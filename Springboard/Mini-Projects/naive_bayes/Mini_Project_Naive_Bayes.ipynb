{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Classification with Naive Bayes\n",
    "***\n",
    "In the mini-project, you'll learn the basics of text analysis using a subset of movie reviews from the rotten tomatoes database. You'll also use a fundamental technique in Bayesian inference, called Naive Bayes. This mini-project is based on [Lab 10 of Harvard's CS109](https://github.com/cs109/2015lab10) class.  Please free to go to the original lab for additional exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from six.moves import range\n",
    "\n",
    "# Setup Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Rotten Tomatoes Dataset](#Rotten-Tomatoes-Dataset)\n",
    "    * [Explore](#Explore)\n",
    "* [The Vector Space Model and a Search Engine](#The-Vector-Space-Model-and-a-Search-Engine)\n",
    "    * [In Code](#In-Code)\n",
    "* [Naive Bayes](#Naive-Bayes)\n",
    "    * [Multinomial Naive Bayes and Other Likelihood Functions](#Multinomial-Naive-Bayes-and-Other-Likelihood-Functions)\n",
    "    * [Picking Hyperparameters for Naive Bayes and Text Maintenance](#Picking-Hyperparameters-for-Naive-Bayes-and-Text-Maintenance)\n",
    "* [Interpretation](#Interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
       "1         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
       "2     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
       "3         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
       "4       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
       "5  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics = pd.read_csv('/Users/luca/anaconda2/Springboard/data/critics.csv')\n",
    "#let's drop rows with missing quotes\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 15561\n",
      "Number of critics: 623\n",
      "Number of movies:  1921\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(critics)\n",
    "n_movies = critics.rtid.unique().size\n",
    "n_critics = critics.critic.unique().size\n",
    "\n",
    "\n",
    "print(\"Number of reviews: {:d}\".format(n_reviews))\n",
    "print(\"Number of critics: {:d}\".format(n_critics))\n",
    "print(\"Number of movies:  {:d}\".format(n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEVCAYAAAAckrn/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUA/HQQEfhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c487d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = critics.copy()\n",
    "df['fresh'] = df.fresh == 'fresh'\n",
    "grp = df.groupby('critic')\n",
    "counts = grp.critic.count()  # number of reviews by each critic\n",
    "means = grp.fresh.mean()     # average freshness for each critic\n",
    "\n",
    "means[counts > 100].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average Rating per critic\")\n",
    "plt.ylabel(\"Number of Critics\")\n",
    "plt.yticks([0, 2, 4, 6, 8, 10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set I</h3>\n",
    "<br/>\n",
    "<b>Exercise:</b> Look at the histogram above. Tell a story about the average ratings per critic. What shape does the distribution look like? What is interesting about the distribution? What might explain these interesting things?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram shapes can be influenced by the bin number. Here for example, a bin of 5 instead of 10 looks much closer to normal through slightly heavy to the right side, but one of 100 looks much less like a normal distribution. Taking this histogram at face value, we see that it looks mostly normal as well through still heavy on the right side. We draw from it a few interesting suppositions about critics. Critics like to give positive ratings but not too positive. You can see that movies rated at .5 or below occur much less often. A large number occur between .6 and .7 and much fewer get above .7 marks. Critics are writers writing to a very broad audience. An audience who might not continue to read thier reviews if they don't agree with the critic more often than not. Hence, critics seem to sparingly give out either high or low ratings. Now critics might hide or subdue thier true rating or preference of a movie but does their crititic reflect that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model and a Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the diagrams here are snipped from [*Introduction to Information Retrieval* by Manning et. al.]( http://nlp.stanford.edu/IR-book/) which is a great resource on text processing. For additional information on text mining and natural language processing, see [*Foundations of Statistical Natural Language Processing* by Manning and Schutze](http://nlp.stanford.edu/fsnlp/).\n",
    "\n",
    "Also check out Python packages [`nltk`](http://www.nltk.org/), [`spaCy`](https://spacy.io/), [`pattern`](http://www.clips.ua.ac.be/pattern), and their associated resources. Also see [`word2vec`](https://en.wikipedia.org/wiki/Word2vec).\n",
    "\n",
    "Let us define the vector derived from document $d$ by $\\bar V(d)$. What does this mean? Each document is treated as a vector containing information about the words contained in it. Each vector has the same length and each entry \"slot\" in the vector contains some kind of data about the words that appear in the document such as presence/absence (1/0), count (an integer) or some other statistic. Each vector has the same length because each document shared the same vocabulary across the full collection of documents -- this collection is called a *corpus*.\n",
    "\n",
    "To define the vocabulary, we take a union of all words we have seen in all documents. We then just associate an array index with them. So \"hello\" may be at index 5 and \"world\" at index 99.\n",
    "\n",
    "Suppose we have the following corpus:\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them.`\n",
    "\n",
    "Suppose we treat each sentence as a document $d$. The vocabulary (often called the *lexicon*) is the following:\n",
    "\n",
    "$V = \\left\\{\\right.$ `a, along, and, as, at, beautiful, branches, bunch, burst, day, fox, fox's, from, gazed, grapes, hanging, he, juice, longingly, mouth, of, one, ready, ripe, seemed, spied, the, them, to, trained, tree, vine, watered, with`$\\left.\\right\\}$\n",
    "\n",
    "Then the document\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree`\n",
    "\n",
    "may be represented as the following sparse vector of word counts:\n",
    "\n",
    "$$\\bar V(d) = \\left( 4,1,0,0,0,1,1,1,0,1,1,0,1,0,1,1,0,0,0,0,2,1,0,1,0,0,1,0,0,0,1,1,0,0 \\right)$$\n",
    "\n",
    "or more succinctly as\n",
    "\n",
    "`[(0, 4), (1, 1), (5, 1), (6, 1), (7, 1), (9, 1), (10, 1), (12, 1), (14, 1), (15, 1), (20, 2), (21, 1), (23, 1),`\n",
    "`(26, 1), (30, 1), (31, 1)]`\n",
    "\n",
    "along with a dictionary\n",
    "\n",
    "``\n",
    "{\n",
    "    0: a, 1: along, 5: beautiful, 6: branches, 7: bunch, 9: day, 10: fox, 12: from, 14: grapes, \n",
    "    15: hanging, 19: mouth, 20: of, 21: one, 23: ripe, 24: seemed, 25: spied, 26: the, \n",
    "    30: tree, 31: vine, \n",
    "}\n",
    "``\n",
    "\n",
    "Then, a set of documents becomes, in the usual `sklearn` style, a sparse matrix with rows being sparse arrays representing documents and columns representing the features/words in the vocabulary.\n",
    "\n",
    "Notice that this representation loses the relative ordering of the terms in the document. That is \"cat ate rat\" and \"rat ate cat\" are the same. Thus, this representation is also known as the Bag-Of-Words representation.\n",
    "\n",
    "Here is another example, from the book quoted above, although the matrix is transposed here so that documents are columns:\n",
    "\n",
    "![novel terms](terms.png)\n",
    "\n",
    "Such a matrix is also catted a Term-Document Matrix. Here, the terms being indexed could be stemmed before indexing; for instance, `jealous` and `jealousy` after stemming are the same feature. One could also make use of other \"Natural Language Processing\" transformations in constructing the vocabulary. We could use Lemmatization, which reduces words to lemmas: work, working, worked would all reduce to work. We could remove \"stopwords\" from our vocabulary, such as common words like \"the\". We could look for particular parts of speech, such as adjectives. This is often done in Sentiment Analysis. And so on. It all depends on our application.\n",
    "\n",
    "From the book:\n",
    ">The standard way of quantifying the similarity between two documents $d_1$ and $d_2$  is to compute the cosine similarity of their vector representations $\\bar V(d_1)$ and $\\bar V(d_2)$:\n",
    "\n",
    "$$S_{12} = \\frac{\\bar V(d_1) \\cdot \\bar V(d_2)}{|\\bar V(d_1)| \\times |\\bar V(d_2)|}$$\n",
    "\n",
    "![Vector Space Model](vsm.png)\n",
    "\n",
    "\n",
    ">There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q = jealous gossip. This query turns into the unit vector $\\bar V(q)$ = (0, 0.707, 0.707) on the three coordinates below. \n",
    "\n",
    "![novel terms](terms2.png)\n",
    "\n",
    ">The key idea now: to assign to each document d a score equal to the dot product:\n",
    "\n",
    "$$\\bar V(q) \\cdot \\bar V(d)$$\n",
    "\n",
    "Then we can use this simple Vector Model as a Search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is\n",
      "Hop on pop\n",
      "Hop off pop\n",
      "Hop Hop hop\n",
      "\n",
      "Transformed text vector is \n",
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [3 0 0 0]]\n",
      "\n",
      "Words for each feature:\n",
      "[u'hop', u'off', u'on', u'pop']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['Hop on pop', 'Hop off pop', 'Hop Hop hop']\n",
    "print(\"Original text is\\n{}\".format('\\n'.join(text)))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Transformed text vector is \\n{}\".format(x))\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print(\"\")\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
    "# just their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy(critics, vectorizer=None):\n",
    "    #Your code here    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(critics.quote)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (critics.fresh == 'fresh').values.astype(np.int)\n",
    "    return X, y\n",
    "X, y = make_xy(critics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes' Theorem, we have that\n",
    "\n",
    "$$P(c \\vert f) = \\frac{P(c \\cap f)}{P(f)}$$\n",
    "\n",
    "where $c$ represents a *class* or category, and $f$ represents a feature vector, such as $\\bar V(d)$ as above. **We are computing the probability that a document (or whatever we are classifying) belongs to category *c* given the features in the document.** $P(f)$ is really just a normalization constant, so the literature usually writes Bayes' Theorem in context of Naive Bayes as\n",
    "\n",
    "$$P(c \\vert f) \\propto P(f \\vert c) P(c) $$\n",
    "\n",
    "$P(c)$ is called the *prior* and is simply the probability of seeing class $c$. But what is $P(f \\vert c)$? This is the probability that we see feature set $f$ given that this document is actually in class $c$. This is called the *likelihood* and comes from the data. One of the major assumptions of the Naive Bayes model is that the features are *conditionally independent* given the class. While the presence of a particular discriminative word may uniquely identify the document as being part of class $c$ and thus violate general feature independence, conditional independence means that the presence of that term is independent of all the other words that appear *within that class*. This is a very important distinction. Recall that if two events are independent, then:\n",
    "\n",
    "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
    "\n",
    "Thus, conditional independence implies\n",
    "\n",
    "$$P(f \\vert c)  = \\prod_i P(f_i | c) $$\n",
    "\n",
    "where $f_i$ is an individual feature (a word in this example).\n",
    "\n",
    "To make a classification, we then choose the class $c$ such that $P(c \\vert f)$ is maximal.\n",
    "\n",
    "There is a small caveat when computing these probabilities. For [floating point underflow](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html) we change the product into a sum by going into log space. This is called the LogSumExp trick. So:\n",
    "\n",
    "$$\\log P(f \\vert c)  = \\sum_i \\log P(f_i \\vert c) $$\n",
    "\n",
    "There is another caveat. What if we see a term that didn't exist in the training data? This means that $P(f_i \\vert c) = 0$ for that term, and thus $P(f \\vert c)  = \\prod_i P(f_i | c) = 0$, which doesn't help us at all. Instead of using zeros, we add a small negligible value called $\\alpha$ to each count. This is called Laplace Smoothing.\n",
    "\n",
    "$$P(f_i \\vert c) = \\frac{N_{ic}+\\alpha}{N_c + \\alpha N_i}$$\n",
    "\n",
    "where $N_{ic}$ is the number of times feature $i$ was seen in class $c$, $N_c$ is the number of times class $c$ was seen and $N_i$ is the number of times feature $i$ was seen globally. $\\alpha$ is sometimes called a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes and Other Likelihood Functions\n",
    "\n",
    "Since we are modeling word counts, we are using variation of Naive Bayes called Multinomial Naive Bayes. This is because the likelihood function actually takes the form of the multinomial distribution.\n",
    "\n",
    "$$P(f \\vert c) = \\frac{\\left( \\sum_i f_i \\right)!}{\\prod_i f_i!} \\prod_{f_i} P(f_i \\vert c)^{f_i} \\propto \\prod_{i} P(f_i \\vert c)$$\n",
    "\n",
    "where the nasty term out front is absorbed as a normalization constant such that probabilities sum to 1.\n",
    "\n",
    "There are many other variations of Naive Bayes, all which depend on what type of value $f_i$ takes. If $f_i$ is continuous, we may be able to use *Gaussian Naive Bayes*. First compute the mean and variance for each class $c$. Then the likelihood, $P(f \\vert c)$ is given as follows\n",
    "\n",
    "$$P(f_i = v \\vert c) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_c}} e^{- \\frac{\\left( v - \\mu_c \\right)^2}{2 \\sigma^2_c}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set II</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Implement a simple Naive Bayes classifier:</p>\n",
    "\n",
    "<ol>\n",
    "<li> split the data set into a training and test set\n",
    "<li> Use `scikit-learn`'s `MultinomialNB()` classifier with default parameters.\n",
    "<li> train the classifier over the training set and test on the test set\n",
    "<li> print the accuracy scores for both the training and the test sets\n",
    "</ol>\n",
    "\n",
    "What do you notice? Is this a good classifier? If not, why not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.92\n",
      "Accuracy on test data:     0.78\n"
     ]
    }
   ],
   "source": [
    "#your turn\n",
    "\n",
    "#import machine learning modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#transform data into vectors\n",
    "x,y=make_xy(critics)\n",
    "\n",
    "# split into training and test sets\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=.3, random_state=42)\n",
    "\n",
    "#initiate the naive bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit( x_train,y_train)\n",
    "\n",
    "#print the accuracy on the training data\n",
    "training_accuracy = clf.score(x_train, y_train)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "\n",
    "#print the accuracy on the test data\n",
    "test_accuracy = clf.score(x_test, y_test)\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit on the training data is high compared to the test dats, therefore, I would suspect that the model is overfitting. However, the prediction on the test set is not bad. We do predict 78% correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Hyperparameters for Naive Bayes and Text Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what value to use for $\\alpha$, and we also need to know which words to include in the vocabulary. As mentioned earlier, some words are obvious stopwords. Other words appear so infrequently that they serve as noise, and other words in addition to stopwords appear so frequently that they may also serve as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find an appropriate value for `min_df` for the `CountVectorizer`. `min_df` can be either an integer or a float/decimal. If it is an integer, `min_df` represents the minimum number of documents a word must appear in for it to be included in the vocabulary. If it is a float, it represents the minimum *percentage* of documents a word must appear in to be included in the vocabulary. From the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set III</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Construct the cumulative distribution of document frequencies (df). The $x$-axis is a document count $x_i$ and the $y$-axis is the percentage of words that appear less than $x_i$ times. For example, at $x=5$, plot a point representing the percentage or number of words that appear in 5 or fewer documents.</p>\n",
    "\n",
    "<p><b>Exercise:</b> Look for the point at which the curve begins climbing steeply. This may be a good value for `min_df`. If we were interested in also picking `max_df`, we would likely pick the value where the curve starts to plateau. What value did you choose?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(critics.quote)\n",
    "df=vectorizer.fit_transform(critics.quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors=df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  1, 10, ...,  3,  1,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq=df.toarray().sum(axis=0)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295413"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 9552,\n",
       "         2: 3486,\n",
       "         3: 1916,\n",
       "         4: 1259,\n",
       "         5: 896,\n",
       "         6: 634,\n",
       "         7: 532,\n",
       "         8: 415,\n",
       "         9: 354,\n",
       "         10: 294,\n",
       "         11: 235,\n",
       "         12: 212,\n",
       "         13: 185,\n",
       "         14: 176,\n",
       "         15: 135,\n",
       "         16: 104,\n",
       "         17: 106,\n",
       "         18: 90,\n",
       "         19: 95,\n",
       "         20: 77,\n",
       "         21: 83,\n",
       "         22: 75,\n",
       "         23: 57,\n",
       "         24: 48,\n",
       "         25: 53,\n",
       "         26: 53,\n",
       "         27: 42,\n",
       "         28: 48,\n",
       "         29: 44,\n",
       "         30: 45,\n",
       "         31: 35,\n",
       "         32: 28,\n",
       "         33: 34,\n",
       "         34: 36,\n",
       "         35: 26,\n",
       "         36: 27,\n",
       "         37: 21,\n",
       "         38: 18,\n",
       "         39: 25,\n",
       "         40: 19,\n",
       "         41: 20,\n",
       "         42: 23,\n",
       "         43: 18,\n",
       "         44: 21,\n",
       "         45: 17,\n",
       "         46: 17,\n",
       "         47: 20,\n",
       "         48: 10,\n",
       "         49: 17,\n",
       "         50: 12,\n",
       "         51: 7,\n",
       "         52: 13,\n",
       "         53: 20,\n",
       "         54: 6,\n",
       "         55: 10,\n",
       "         56: 17,\n",
       "         57: 14,\n",
       "         58: 10,\n",
       "         59: 10,\n",
       "         60: 13,\n",
       "         61: 11,\n",
       "         62: 10,\n",
       "         63: 11,\n",
       "         64: 9,\n",
       "         65: 4,\n",
       "         66: 12,\n",
       "         67: 12,\n",
       "         68: 7,\n",
       "         69: 3,\n",
       "         70: 3,\n",
       "         71: 6,\n",
       "         72: 5,\n",
       "         73: 6,\n",
       "         74: 6,\n",
       "         75: 5,\n",
       "         76: 11,\n",
       "         77: 11,\n",
       "         78: 2,\n",
       "         79: 4,\n",
       "         80: 11,\n",
       "         81: 5,\n",
       "         82: 5,\n",
       "         83: 5,\n",
       "         84: 7,\n",
       "         85: 4,\n",
       "         86: 5,\n",
       "         87: 3,\n",
       "         88: 2,\n",
       "         89: 4,\n",
       "         90: 3,\n",
       "         91: 4,\n",
       "         92: 1,\n",
       "         93: 8,\n",
       "         94: 5,\n",
       "         95: 5,\n",
       "         96: 4,\n",
       "         97: 3,\n",
       "         98: 2,\n",
       "         99: 3,\n",
       "         100: 5,\n",
       "         101: 4,\n",
       "         102: 2,\n",
       "         103: 3,\n",
       "         104: 2,\n",
       "         105: 1,\n",
       "         106: 8,\n",
       "         107: 4,\n",
       "         108: 2,\n",
       "         110: 3,\n",
       "         111: 2,\n",
       "         112: 6,\n",
       "         113: 4,\n",
       "         116: 8,\n",
       "         117: 2,\n",
       "         118: 3,\n",
       "         119: 1,\n",
       "         121: 2,\n",
       "         122: 1,\n",
       "         123: 3,\n",
       "         124: 1,\n",
       "         126: 1,\n",
       "         127: 2,\n",
       "         128: 4,\n",
       "         130: 1,\n",
       "         131: 1,\n",
       "         133: 2,\n",
       "         134: 3,\n",
       "         135: 1,\n",
       "         137: 2,\n",
       "         138: 4,\n",
       "         140: 1,\n",
       "         141: 3,\n",
       "         142: 4,\n",
       "         144: 3,\n",
       "         145: 2,\n",
       "         146: 3,\n",
       "         147: 3,\n",
       "         149: 1,\n",
       "         150: 1,\n",
       "         151: 1,\n",
       "         153: 1,\n",
       "         154: 1,\n",
       "         156: 3,\n",
       "         157: 2,\n",
       "         158: 2,\n",
       "         159: 2,\n",
       "         160: 2,\n",
       "         161: 2,\n",
       "         162: 1,\n",
       "         166: 3,\n",
       "         169: 1,\n",
       "         170: 2,\n",
       "         172: 2,\n",
       "         175: 1,\n",
       "         176: 1,\n",
       "         177: 3,\n",
       "         178: 1,\n",
       "         179: 1,\n",
       "         182: 1,\n",
       "         183: 1,\n",
       "         184: 1,\n",
       "         186: 3,\n",
       "         188: 1,\n",
       "         189: 1,\n",
       "         191: 1,\n",
       "         192: 2,\n",
       "         195: 1,\n",
       "         196: 2,\n",
       "         198: 1,\n",
       "         199: 1,\n",
       "         200: 1,\n",
       "         201: 2,\n",
       "         202: 1,\n",
       "         205: 1,\n",
       "         207: 1,\n",
       "         208: 2,\n",
       "         209: 1,\n",
       "         210: 1,\n",
       "         212: 1,\n",
       "         213: 1,\n",
       "         214: 2,\n",
       "         219: 2,\n",
       "         221: 1,\n",
       "         222: 1,\n",
       "         223: 1,\n",
       "         225: 1,\n",
       "         226: 1,\n",
       "         227: 3,\n",
       "         231: 2,\n",
       "         232: 2,\n",
       "         233: 1,\n",
       "         234: 1,\n",
       "         236: 1,\n",
       "         237: 2,\n",
       "         239: 1,\n",
       "         242: 1,\n",
       "         244: 3,\n",
       "         248: 1,\n",
       "         249: 1,\n",
       "         252: 3,\n",
       "         254: 1,\n",
       "         256: 1,\n",
       "         257: 3,\n",
       "         258: 3,\n",
       "         260: 1,\n",
       "         261: 1,\n",
       "         263: 1,\n",
       "         264: 1,\n",
       "         266: 1,\n",
       "         271: 1,\n",
       "         272: 1,\n",
       "         273: 1,\n",
       "         274: 1,\n",
       "         276: 1,\n",
       "         278: 1,\n",
       "         279: 1,\n",
       "         282: 1,\n",
       "         284: 2,\n",
       "         287: 1,\n",
       "         288: 1,\n",
       "         290: 1,\n",
       "         293: 1,\n",
       "         295: 1,\n",
       "         296: 2,\n",
       "         301: 1,\n",
       "         305: 1,\n",
       "         306: 1,\n",
       "         320: 1,\n",
       "         322: 1,\n",
       "         324: 1,\n",
       "         330: 1,\n",
       "         336: 1,\n",
       "         341: 1,\n",
       "         343: 1,\n",
       "         344: 1,\n",
       "         354: 1,\n",
       "         363: 1,\n",
       "         364: 1,\n",
       "         367: 1,\n",
       "         369: 1,\n",
       "         370: 1,\n",
       "         383: 1,\n",
       "         386: 1,\n",
       "         387: 1,\n",
       "         388: 1,\n",
       "         396: 1,\n",
       "         399: 1,\n",
       "         413: 1,\n",
       "         415: 1,\n",
       "         417: 1,\n",
       "         422: 1,\n",
       "         433: 1,\n",
       "         434: 1,\n",
       "         438: 2,\n",
       "         439: 1,\n",
       "         443: 1,\n",
       "         462: 1,\n",
       "         469: 1,\n",
       "         472: 1,\n",
       "         478: 1,\n",
       "         496: 1,\n",
       "         504: 1,\n",
       "         507: 1,\n",
       "         531: 1,\n",
       "         540: 1,\n",
       "         545: 2,\n",
       "         572: 1,\n",
       "         591: 1,\n",
       "         599: 1,\n",
       "         615: 1,\n",
       "         644: 1,\n",
       "         657: 1,\n",
       "         663: 1,\n",
       "         688: 1,\n",
       "         689: 1,\n",
       "         691: 1,\n",
       "         702: 1,\n",
       "         703: 1,\n",
       "         711: 1,\n",
       "         744: 1,\n",
       "         790: 1,\n",
       "         821: 1,\n",
       "         864: 1,\n",
       "         875: 1,\n",
       "         887: 1,\n",
       "         993: 1,\n",
       "         1018: 1,\n",
       "         1024: 1,\n",
       "         1037: 1,\n",
       "         1069: 1,\n",
       "         1085: 1,\n",
       "         1121: 1,\n",
       "         1157: 1,\n",
       "         1237: 1,\n",
       "         1295: 1,\n",
       "         1301: 1,\n",
       "         1331: 1,\n",
       "         1390: 1,\n",
       "         1408: 2,\n",
       "         1836: 1,\n",
       "         1903: 1,\n",
       "         2148: 1,\n",
       "         2199: 1,\n",
       "         2267: 1,\n",
       "         2335: 1,\n",
       "         2411: 1,\n",
       "         2577: 1,\n",
       "         2858: 1,\n",
       "         3681: 1,\n",
       "         4252: 1,\n",
       "         5332: 1,\n",
       "         5974: 1,\n",
       "         6238: 1,\n",
       "         9778: 1,\n",
       "         9904: 1,\n",
       "         16805: 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_doc=Counter(word_freq)\n",
    "word_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(word_doc.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 16805), (u'and', 9904), (u'of', 9778), (u'to', 6238), (u'is', 5974), (u'it', 5332), (u'in', 4252), (u'that', 3681), (u'as', 2858), (u'with', 2577), (u'but', 2411), (u'this', 2335), (u'film', 2267), (u'movie', 2199), (u'for', 2148), (u'its', 1903), (u'an', 1836), (u'be', 1408), (u'you', 1408), (u'on', 1390), (u'by', 1331), (u'has', 1301), (u'one', 1295), (u'not', 1237), (u'are', 1157), (u'more', 1121), (u'his', 1085), (u'all', 1069), (u'than', 1037), (u'at', 1024), (u'about', 1018), (u'from', 993), (u'have', 887), (u'so', 875), (u'like', 864), (u'most', 821), (u'there', 790), (u'what', 744), (u'if', 711), (u'he', 703), (u'or', 702), (u'good', 691), (u'can', 689), (u'who', 688), (u'up', 663), (u'story', 657), (u'into', 644), (u'much', 615), (u'out', 599), (u'comedy', 591)]\n"
     ]
    }
   ],
   "source": [
    "words = list(vectorizer.get_feature_names())\n",
    "word_count = Counter(dict(zip(words, word_freq)))\n",
    "print (word_count.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295413"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create dict to count word occurance in document(sentences)\n",
    "dict_words={i:0 for i in range(len(df_vectors[0]))}\n",
    "vocab = list(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over words in sentences\n",
    "for i in xrange(len(df_vectors[0])):\n",
    "    \n",
    "    #value holder for word occurances in sentences\n",
    "    y=0\n",
    "    \n",
    "    #loop over document/sentences counting word occurances\n",
    "    #sum up sentence/document one first word occurance with sentence/document two word one...\n",
    "    for j in xrange(len (df_vectors)):\n",
    "        y=y+df_vectors[j][i]\n",
    "        \n",
    "    #save word occurs in documents to dictionary\n",
    "    dict_words[i]=y\n",
    "    \n",
    "    #loop increments to next word to sum over sentences\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use counter to sum word occurance \n",
    "word_pert=Counter(dict_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 9552,\n",
       "         2: 3486,\n",
       "         3: 1916,\n",
       "         4: 1259,\n",
       "         5: 896,\n",
       "         6: 634,\n",
       "         7: 532,\n",
       "         8: 415,\n",
       "         9: 354,\n",
       "         10: 294,\n",
       "         11: 235,\n",
       "         12: 212,\n",
       "         13: 185,\n",
       "         14: 176,\n",
       "         15: 135,\n",
       "         16: 104,\n",
       "         17: 106,\n",
       "         18: 90,\n",
       "         19: 95,\n",
       "         20: 77,\n",
       "         21: 83,\n",
       "         22: 75,\n",
       "         23: 57,\n",
       "         24: 48,\n",
       "         25: 53,\n",
       "         26: 53,\n",
       "         27: 42,\n",
       "         28: 48,\n",
       "         29: 44,\n",
       "         30: 45,\n",
       "         31: 35,\n",
       "         32: 28,\n",
       "         33: 34,\n",
       "         34: 36,\n",
       "         35: 26,\n",
       "         36: 27,\n",
       "         37: 21,\n",
       "         38: 18,\n",
       "         39: 25,\n",
       "         40: 19,\n",
       "         41: 20,\n",
       "         42: 23,\n",
       "         43: 18,\n",
       "         44: 21,\n",
       "         45: 17,\n",
       "         46: 17,\n",
       "         47: 20,\n",
       "         48: 10,\n",
       "         49: 17,\n",
       "         50: 12,\n",
       "         51: 7,\n",
       "         52: 13,\n",
       "         53: 20,\n",
       "         54: 6,\n",
       "         55: 10,\n",
       "         56: 17,\n",
       "         57: 14,\n",
       "         58: 10,\n",
       "         59: 10,\n",
       "         60: 13,\n",
       "         61: 11,\n",
       "         62: 10,\n",
       "         63: 11,\n",
       "         64: 9,\n",
       "         65: 4,\n",
       "         66: 12,\n",
       "         67: 12,\n",
       "         68: 7,\n",
       "         69: 3,\n",
       "         70: 3,\n",
       "         71: 6,\n",
       "         72: 5,\n",
       "         73: 6,\n",
       "         74: 6,\n",
       "         75: 5,\n",
       "         76: 11,\n",
       "         77: 11,\n",
       "         78: 2,\n",
       "         79: 4,\n",
       "         80: 11,\n",
       "         81: 5,\n",
       "         82: 5,\n",
       "         83: 5,\n",
       "         84: 7,\n",
       "         85: 4,\n",
       "         86: 5,\n",
       "         87: 3,\n",
       "         88: 2,\n",
       "         89: 4,\n",
       "         90: 3,\n",
       "         91: 4,\n",
       "         92: 1,\n",
       "         93: 8,\n",
       "         94: 5,\n",
       "         95: 5,\n",
       "         96: 4,\n",
       "         97: 3,\n",
       "         98: 2,\n",
       "         99: 3,\n",
       "         100: 5,\n",
       "         101: 4,\n",
       "         102: 2,\n",
       "         103: 3,\n",
       "         104: 2,\n",
       "         105: 1,\n",
       "         106: 8,\n",
       "         107: 4,\n",
       "         108: 2,\n",
       "         110: 3,\n",
       "         111: 2,\n",
       "         112: 6,\n",
       "         113: 4,\n",
       "         116: 8,\n",
       "         117: 2,\n",
       "         118: 3,\n",
       "         119: 1,\n",
       "         121: 2,\n",
       "         122: 1,\n",
       "         123: 3,\n",
       "         124: 1,\n",
       "         126: 1,\n",
       "         127: 2,\n",
       "         128: 4,\n",
       "         130: 1,\n",
       "         131: 1,\n",
       "         133: 2,\n",
       "         134: 3,\n",
       "         135: 1,\n",
       "         137: 2,\n",
       "         138: 4,\n",
       "         140: 1,\n",
       "         141: 3,\n",
       "         142: 4,\n",
       "         144: 3,\n",
       "         145: 2,\n",
       "         146: 3,\n",
       "         147: 3,\n",
       "         149: 1,\n",
       "         150: 1,\n",
       "         151: 1,\n",
       "         153: 1,\n",
       "         154: 1,\n",
       "         156: 3,\n",
       "         157: 2,\n",
       "         158: 2,\n",
       "         159: 2,\n",
       "         160: 2,\n",
       "         161: 2,\n",
       "         162: 1,\n",
       "         166: 3,\n",
       "         169: 1,\n",
       "         170: 2,\n",
       "         172: 2,\n",
       "         175: 1,\n",
       "         176: 1,\n",
       "         177: 3,\n",
       "         178: 1,\n",
       "         179: 1,\n",
       "         182: 1,\n",
       "         183: 1,\n",
       "         184: 1,\n",
       "         186: 3,\n",
       "         188: 1,\n",
       "         189: 1,\n",
       "         191: 1,\n",
       "         192: 2,\n",
       "         195: 1,\n",
       "         196: 2,\n",
       "         198: 1,\n",
       "         199: 1,\n",
       "         200: 1,\n",
       "         201: 2,\n",
       "         202: 1,\n",
       "         205: 1,\n",
       "         207: 1,\n",
       "         208: 2,\n",
       "         209: 1,\n",
       "         210: 1,\n",
       "         212: 1,\n",
       "         213: 1,\n",
       "         214: 2,\n",
       "         219: 2,\n",
       "         221: 1,\n",
       "         222: 1,\n",
       "         223: 1,\n",
       "         225: 1,\n",
       "         226: 1,\n",
       "         227: 3,\n",
       "         231: 2,\n",
       "         232: 2,\n",
       "         233: 1,\n",
       "         234: 1,\n",
       "         236: 1,\n",
       "         237: 2,\n",
       "         239: 1,\n",
       "         242: 1,\n",
       "         244: 3,\n",
       "         248: 1,\n",
       "         249: 1,\n",
       "         252: 3,\n",
       "         254: 1,\n",
       "         256: 1,\n",
       "         257: 3,\n",
       "         258: 3,\n",
       "         260: 1,\n",
       "         261: 1,\n",
       "         263: 1,\n",
       "         264: 1,\n",
       "         266: 1,\n",
       "         271: 1,\n",
       "         272: 1,\n",
       "         273: 1,\n",
       "         274: 1,\n",
       "         276: 1,\n",
       "         278: 1,\n",
       "         279: 1,\n",
       "         282: 1,\n",
       "         284: 2,\n",
       "         287: 1,\n",
       "         288: 1,\n",
       "         290: 1,\n",
       "         293: 1,\n",
       "         295: 1,\n",
       "         296: 2,\n",
       "         301: 1,\n",
       "         305: 1,\n",
       "         306: 1,\n",
       "         320: 1,\n",
       "         322: 1,\n",
       "         324: 1,\n",
       "         330: 1,\n",
       "         336: 1,\n",
       "         341: 1,\n",
       "         343: 1,\n",
       "         344: 1,\n",
       "         354: 1,\n",
       "         363: 1,\n",
       "         364: 1,\n",
       "         367: 1,\n",
       "         369: 1,\n",
       "         370: 1,\n",
       "         383: 1,\n",
       "         386: 1,\n",
       "         387: 1,\n",
       "         388: 1,\n",
       "         396: 1,\n",
       "         399: 1,\n",
       "         413: 1,\n",
       "         415: 1,\n",
       "         417: 1,\n",
       "         422: 1,\n",
       "         433: 1,\n",
       "         434: 1,\n",
       "         438: 2,\n",
       "         439: 1,\n",
       "         443: 1,\n",
       "         462: 1,\n",
       "         469: 1,\n",
       "         472: 1,\n",
       "         478: 1,\n",
       "         496: 1,\n",
       "         504: 1,\n",
       "         507: 1,\n",
       "         531: 1,\n",
       "         540: 1,\n",
       "         545: 2,\n",
       "         572: 1,\n",
       "         591: 1,\n",
       "         599: 1,\n",
       "         615: 1,\n",
       "         644: 1,\n",
       "         657: 1,\n",
       "         663: 1,\n",
       "         688: 1,\n",
       "         689: 1,\n",
       "         691: 1,\n",
       "         702: 1,\n",
       "         703: 1,\n",
       "         711: 1,\n",
       "         744: 1,\n",
       "         790: 1,\n",
       "         821: 1,\n",
       "         864: 1,\n",
       "         875: 1,\n",
       "         887: 1,\n",
       "         993: 1,\n",
       "         1018: 1,\n",
       "         1024: 1,\n",
       "         1037: 1,\n",
       "         1069: 1,\n",
       "         1085: 1,\n",
       "         1121: 1,\n",
       "         1157: 1,\n",
       "         1237: 1,\n",
       "         1295: 1,\n",
       "         1301: 1,\n",
       "         1331: 1,\n",
       "         1390: 1,\n",
       "         1408: 2,\n",
       "         1836: 1,\n",
       "         1903: 1,\n",
       "         2148: 1,\n",
       "         2199: 1,\n",
       "         2267: 1,\n",
       "         2335: 1,\n",
       "         2411: 1,\n",
       "         2577: 1,\n",
       "         2858: 1,\n",
       "         3681: 1,\n",
       "         4252: 1,\n",
       "         5332: 1,\n",
       "         5974: 1,\n",
       "         6238: 1,\n",
       "         9778: 1,\n",
       "         9904: 1,\n",
       "         16805: 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#should be same as word_doc\n",
    "word_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want freq of words that appear in i documents (for x=5 that is words that appear in 5 documents)\n",
    "#empty variable for counting\n",
    "x=0\n",
    "\n",
    "#list of words per document for plotting later\n",
    "x_points=[]\n",
    "\n",
    "#list to hold word document counts and initialize with a zero for plotting later\n",
    "p_pert=[]\n",
    "\n",
    "#loop over \n",
    "for i in word_doc.values():\n",
    "    x+=i\n",
    "    p=x\n",
    "    x_points.append(x)\n",
    "    p_pert.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAE9CAYAAABp1zkEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+v9Cw33Zr/B4mQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108871190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(p_pert)\n",
    "fig, ax =plt.subplots()\n",
    "x=np.linspace(0,316,num=n)\n",
    "ax.plot(x,p_pert)\n",
    "#_ = plt.xlim(0,316)\n",
    "_ = ax.set_xlabel('document count  Xi')\n",
    "_ = ax.set_ylabel('Counts')\n",
    "_ = ax.set_title('Cumulative Distribution Functions\\nNumber of words that appear less than  Xi times')\n",
    "\n",
    "\n",
    "inset_ax=fig.add_axes([0.55,.3,.35,.35])\n",
    "inset_ax.plot(x,p_pert)\n",
    "inset_ax.set_title('zoom in on left side')\n",
    "inset_ax.set_frame_on('on')\n",
    "\n",
    "inset_ax.set_xlim(0,40)\n",
    "plt.margins(0.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Look for the point at which the curve begins climbing steeply. This may be a good value for min_df. If we were interested in also picking max_df, we would likely pick the value where the curve starts to plateau. What value did you choose?\n",
    "\n",
    "\n",
    "Here we are looking at counts of words that appear in a document. X=1 then is all words that appear in exactly one document. In our data, words that appear in one document number 9552. We see that the data stabilizes after 10 documents for which words appear in. I, therefore, would recommend picking 10 as the min_df. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\alpha$ is chosen to be a small value that simply avoids having zeros in the probability computations. This value can sometimes be chosen arbitrarily with domain expertise, but we will use K-fold cross validation. In K-fold cross-validation, we divide the data into $K$ non-overlapping parts. We train on $K-1$ of the folds and test on the remaining fold. We then iterate, so that each fold serves as the test fold exactly once. The function `cv_score` performs the K-fold cross-validation algorithm for us, but we need to pass a function that measures the performance of the algorithm on each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-likelihood as the score here in `scorefunc`. The higher the log-likelihood, the better. Indeed, what we do in `cv_score` above is to implement the cross-validation part of `GridSearchCV`.\n",
    "\n",
    "The custom scoring function `scorefunc` allows us to use different metrics depending on the decision risk we care about (precision, accuracy, profit etc.) directly on the validation set. You will often find people using `roc_auc`, precision, recall, or `F1-score` as the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross-validate over the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the train and test masks first, and then we can run the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\n",
    "mask = np.zeros(critics.shape[0], dtype=np.bool)\n",
    "mask[itest] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set IV</h3>\n",
    "\n",
    "<p><b>Exercise:</b> What does using the function `log_likelihood` as the score mean? What are we trying to optimize for?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Without writing any code, what do you think would happen if you choose a value of $\\alpha$ that is too high?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Using the skeleton code below, find the best values of the parameter `alpha`, and use the value of `min_df` you chose in the previous exercise set. Use the `cv_score` function above with the `log_likelihood` function for scoring.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to optimize for the parameter selection. We want the maximum likelihood of data with respect to our parameters. A maximum likelihood selects the parameter so that the data is most likely to occur. Using the log_likelihood allows us to use the sum instead of the product. Two benefits of this is, it of keeping us from running out of floating point precision since the product gets very small quickly, and the computation is easier. \n",
    "\n",
    "If you choose an alpha that is to high, then your estimation/prediction is giving excess weight to items that did not occur in your data. You are in essense saying that what was not seen is much more likely to occur than what was seen. Your predictions will be much worse in the end. (Alpha is the smoothing term here)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score of -562.168726231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "best_min_df = 20 # YOUR TURN: put your value of min_df here.\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df)       \n",
    "    Xthis, ythis = make_xy(critics, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    # your turn\n",
    "    #initiate the naive bayes classifier\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    #call the score function to fit and evaluate\n",
    "    cross_score =  cv_score(clf,Xtrainthis,ytrainthis,log_likelihood)\n",
    "    \n",
    "    #save best score for comparsion in next loop\n",
    "    if cross_score>maxscore:\n",
    "        maxscore=cross_score\n",
    "        best_alpha=alpha\n",
    "#print best of the results        \n",
    "print(\"The best score of {}\".format(maxscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"alpha: {}\".format(best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set V: Working with the Best Parameters</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Using the best value of  `alpha` you just found, calculate the accuracy on the training and test sets. Is this classifier better? Why (not)?</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.786250\n",
      "Accuracy on test data:     0.712817\n"
     ]
    }
   ],
   "source": [
    "n_vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, y = make_xy(critics, n_vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "#your turn. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1979 2333]\n",
      " [ 795 5785]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, clf.predict(xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this is a better classifier. Previously, we got accuracy on training data at .92 and test data at .72. Here, we got accuracy on training data a .79 and test data at .71. This is better since we are not overfitting. Overfitting is the use of noise in the data to model the prediction. You can see this by the difference in test accuracy compared to training accuracy. The first case had a difference of .2 compared with .07 this time. The second model is using much less noise to make predictions thus its predictions can be considered to be more reliable even through the score is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the strongly predictive features?\n",
    "\n",
    "We use a neat trick to identify strongly predictive features (i.e. words). \n",
    "\n",
    "* first, create a data set such that each row has exactly one feature. This is represented by the identity matrix.\n",
    "* use the trained classifier to make predictions on this matrix\n",
    "* sort the rows by predicted probabilities, and pick the top and bottom $K$ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "('        entertaining', '0.88')\n",
      "('               smart', '0.88')\n",
      "('            touching', '0.86')\n",
      "('         intelligent', '0.86')\n",
      "('         masterpiece', '0.85')\n",
      "('              modern', '0.85')\n",
      "('         performance', '0.84')\n",
      "('          remarkable', '0.84')\n",
      "('               years', '0.84')\n",
      "('             perfect', '0.83')\n",
      "Bad words\t     P(fresh | word)\n",
      "('              stupid', '0.29')\n",
      "('           pointless', '0.29')\n",
      "('              sitcom', '0.29')\n",
      "('              unless', '0.28')\n",
      "('      disappointment', '0.28')\n",
      "('                lame', '0.28')\n",
      "('               sadly', '0.26')\n",
      "('                dull', '0.24')\n",
      "('               bland', '0.21')\n",
      "('       unfortunately', '0.19')\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(fresh | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(fresh | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VI</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Why does this method work? What does the probability for each row in the identity matrix represent</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of each row is the probility that the feature/word will be in the catagory fresh (recieve a good rating by the critic). This probability also tells us which features/words our classifer is using to determine if a critics review will be 'fresh'. It works because the Naive Bayes' pred_proba returns the probability for each word/feature set given the class/document, P(f|c), i.e. the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above exercise is an example of *feature selection*. There are many other feature selection methods. A list of feature selection methods available in `sklearn` is [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection). The most common feature selection technique for text mining is the chi-squared $\\left( \\chi^2 \\right)$ [method](http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Errors\n",
    "\n",
    "We can see mis-predictions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mis-predicted Rotten quotes\n",
      "---------------------------\n",
      "Walken is one of the few undeniably charismatic male villains of recent years; he can generate a snakelike charm that makes his worst characters the most memorable, and here he operates on pure style.\n",
      "\n",
      "It is sometimes funny in a puzzling kind of way, it is generally overwrought in an irritating kind of way, and once in a while it is inappropriately touching.\n",
      "\n",
      "The hip, smart yarn has a bite not seen in American movies since The War of the Roses.\n",
      "\n",
      "Part comedy, part family drama, part romance, part special-effects mystery-adventure, and not entirely satisfying on any of these levels.\n",
      "\n",
      "Lauded as a witty moral fable with a revelatory performance from its star, this romantic comedy is in fact meretricious, manipulative and reactionary.\n",
      "\n",
      "Mis-predicted Fresh quotes\n",
      "--------------------------\n",
      "It's often impossible to distinguish what's meant to be cartoonish from what's meant to be dramatic, but the confusion seems appropriately adolescent.\n",
      "\n",
      "Might it be a serious attempt to right some unretrievable wrong via gallows humor which avoids the polemics? This seems to be the course taken; the attempt at least can be respected in theory.\n",
      "\n",
      "Though it's a good half hour too long, this overblown 1993 spin-off of the 60s TV show otherwise adds up to a pretty good suspense thriller.\n",
      "\n",
      "If it isn't likely to generate what Mr. Brooks himself refers to as 'Spaceballs II: The Search for More Money, neither is it anything less than gentle, harmless satire that occasionally has real bite.\n",
      "\n",
      "This doesn't usually happen to me, but 15 minutes before the end of Casper I suddenly realized that if I didn't take a deep breath, I was going to start sobbing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = make_xy(critics, vectorizer)\n",
    "\n",
    "prob = clf.predict_proba(x)[:, 0]\n",
    "predict = clf.predict(x)\n",
    "\n",
    "bad_rotten = np.argsort(prob[y == 0])[:5]\n",
    "bad_fresh = np.argsort(prob[y == 1])[-5:]\n",
    "\n",
    "print(\"Mis-predicted Rotten quotes\")\n",
    "print('---------------------------')\n",
    "for row in bad_rotten:\n",
    "    print(critics[y == 0].quote.iloc[row])\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Mis-predicted Fresh quotes\")\n",
    "print('--------------------------')\n",
    "for row in bad_fresh:\n",
    "    print(critics[y == 1].quote.iloc[row])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VII: Predicting the Freshness for a New Review</h3>\n",
    "<br/>\n",
    "<div>\n",
    "<b>Exercise:</b>\n",
    "<ul>\n",
    "<li> Using your best trained classifier, predict the freshness of the following sentence: *'This movie is not remarkable, touching, or superb in any way'*\n",
    "<li> Is the result what you'd expect? Why (not)?\n",
    "</ul>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text=['This movie is not remarkable, touching, or superb in any way']\n",
    "type(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change text into vector\n",
    "new_text=vectorizer.transform(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#make prediciton on new set\n",
    "print(clf.predict(new_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It predicts that this sentence would be classified as 'fresh'. This would be a false positive as we can tell from reading the sentence. It is not too surprising that the classifier gets this wrong. The classifier here is only looking at words that have a high probability of being in the class 'fresh'. This sentence contains at three words we would expect to be highly associated with the 'fresh' class, hence, it predicts it is. The 'not' here does not alert the classifer that these three positive words should be negatively associated resulting in a misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: TF-IDF Weighting for Term Importance\n",
    "\n",
    "TF-IDF stands for \n",
    "\n",
    "`Term-Frequency X Inverse Document Frequency`.\n",
    "\n",
    "In the standard `CountVectorizer` model above, we used just the term frequency in a document of words in our vocabulary. In TF-IDF, we weight this term frequency by the inverse of its popularity in all documents. For example, if the word \"movie\" showed up in all the documents, it would not have much predictive value. It could actually be considered a stopword. By weighing its counts by 1 divided by its overall frequency, we downweight it. We can then use this TF-IDF weighted features as inputs to any classifier. **TF-IDF is essentially a measure of term importance, and of how discriminative a word is in a corpus.** There are a variety of nuances involved in computing TF-IDF, mainly involving where to add the smoothing term to avoid division by 0, or log of 0 errors. The formula for TF-IDF in `scikit-learn` differs from that of most textbooks: \n",
    "\n",
    "$$\\mbox{TF-IDF}(t, d) = \\mbox{TF}(t, d)\\times \\mbox{IDF}(t) = n_{td} \\log{\\left( \\frac{\\vert D \\vert}{\\vert d : t \\in d \\vert} + 1 \\right)}$$\n",
    "\n",
    "where $n_{td}$ is the number of times term $t$ occurs in document $d$, $\\vert D \\vert$ is the number of documents, and $\\vert d : t \\in d \\vert$ is the number of documents that contain $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/dev/modules/feature_extraction.html#text-feature-extraction\n",
    "# http://scikit-learn.org/dev/modules/classes.html#text-feature-extraction-ref\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "Xtfidf=tfidfvectorizer.fit_transform(critics.quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VIII: Enrichment</h3>\n",
    "\n",
    "<p>\n",
    "There are several additional things we could try. Try some of these as exercises:\n",
    "<ol>\n",
    "<li> Build a Naive Bayes model where the features are n-grams instead of words. N-grams are phrases containing n words next to each other: a bigram contains 2 words, a trigram contains 3 words, and 6-gram contains 6 words. This is useful because \"not good\" and \"so good\" mean very different things. On the other hand, as n increases, the model does not scale well since the feature set becomes more sparse.\n",
    "<li> Try a model besides Naive Bayes, one that would allow for interactions between words -- for example, a Random Forest classifier.\n",
    "<li> Try adding supplemental features -- information about genre, director, cast, etc.\n",
    "<li> Use word2vec or [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to group words into topics and use those topics for prediction.\n",
    "<li> Use TF-IDF weighting instead of word counts.\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<b>Exercise:</b> Try a few of these ideas to improve the model (or any other ideas of your own). Implement here and report on the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 28 31 ..., 21 11 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({10: 319,\n",
       "         11: 246,\n",
       "         12: 185,\n",
       "         13: 168,\n",
       "         14: 152,\n",
       "         15: 108,\n",
       "         16: 111,\n",
       "         17: 103,\n",
       "         18: 81,\n",
       "         19: 67,\n",
       "         20: 50,\n",
       "         21: 70,\n",
       "         22: 54,\n",
       "         23: 39,\n",
       "         24: 44,\n",
       "         25: 32,\n",
       "         26: 32,\n",
       "         27: 30,\n",
       "         28: 34,\n",
       "         29: 21,\n",
       "         30: 21,\n",
       "         31: 24,\n",
       "         32: 23,\n",
       "         33: 29,\n",
       "         34: 20,\n",
       "         35: 12,\n",
       "         36: 16,\n",
       "         37: 15,\n",
       "         38: 14,\n",
       "         39: 13,\n",
       "         40: 14,\n",
       "         41: 14,\n",
       "         42: 8,\n",
       "         43: 16,\n",
       "         44: 4,\n",
       "         45: 9,\n",
       "         46: 8,\n",
       "         47: 9,\n",
       "         48: 7,\n",
       "         49: 5,\n",
       "         50: 7,\n",
       "         51: 13,\n",
       "         52: 5,\n",
       "         53: 6,\n",
       "         54: 12,\n",
       "         55: 5,\n",
       "         56: 8,\n",
       "         57: 4,\n",
       "         58: 4,\n",
       "         59: 4,\n",
       "         60: 6,\n",
       "         61: 2,\n",
       "         62: 5,\n",
       "         63: 6,\n",
       "         64: 7,\n",
       "         65: 2,\n",
       "         66: 2,\n",
       "         67: 2,\n",
       "         68: 2,\n",
       "         70: 1,\n",
       "         71: 2,\n",
       "         72: 6,\n",
       "         73: 2,\n",
       "         74: 4,\n",
       "         75: 4,\n",
       "         76: 3,\n",
       "         78: 2,\n",
       "         79: 2,\n",
       "         80: 2,\n",
       "         81: 1,\n",
       "         82: 3,\n",
       "         84: 1,\n",
       "         85: 2,\n",
       "         87: 1,\n",
       "         88: 3,\n",
       "         89: 4,\n",
       "         90: 1,\n",
       "         92: 1,\n",
       "         93: 4,\n",
       "         94: 1,\n",
       "         95: 2,\n",
       "         96: 3,\n",
       "         98: 1,\n",
       "         99: 2,\n",
       "         100: 2,\n",
       "         102: 1,\n",
       "         103: 3,\n",
       "         105: 2,\n",
       "         106: 2,\n",
       "         107: 1,\n",
       "         109: 1,\n",
       "         110: 1,\n",
       "         111: 2,\n",
       "         112: 2,\n",
       "         113: 2,\n",
       "         118: 1,\n",
       "         121: 1,\n",
       "         124: 1,\n",
       "         128: 1,\n",
       "         129: 2,\n",
       "         130: 2,\n",
       "         131: 1,\n",
       "         134: 2,\n",
       "         135: 1,\n",
       "         138: 1,\n",
       "         141: 1,\n",
       "         143: 1,\n",
       "         149: 1,\n",
       "         151: 3,\n",
       "         153: 1,\n",
       "         157: 2,\n",
       "         163: 1,\n",
       "         167: 1,\n",
       "         170: 3,\n",
       "         172: 3,\n",
       "         175: 1,\n",
       "         178: 1,\n",
       "         179: 1,\n",
       "         181: 1,\n",
       "         192: 1,\n",
       "         193: 1,\n",
       "         199: 1,\n",
       "         201: 1,\n",
       "         206: 1,\n",
       "         209: 1,\n",
       "         215: 1,\n",
       "         225: 1,\n",
       "         226: 1,\n",
       "         245: 1,\n",
       "         247: 1,\n",
       "         253: 1,\n",
       "         254: 1,\n",
       "         256: 1,\n",
       "         260: 1,\n",
       "         261: 1,\n",
       "         262: 1,\n",
       "         277: 1,\n",
       "         279: 1,\n",
       "         350: 1,\n",
       "         353: 1,\n",
       "         368: 1,\n",
       "         381: 1,\n",
       "         393: 1,\n",
       "         419: 1,\n",
       "         433: 1,\n",
       "         520: 1,\n",
       "         554: 1,\n",
       "         582: 1,\n",
       "         622: 1,\n",
       "         661: 1,\n",
       "         708: 1,\n",
       "         868: 1,\n",
       "         1010: 1,\n",
       "         2094: 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer ='word',min_df=10, ngram_range=(2, 2))\n",
    "\n",
    "vec.fit(critics.quote)\n",
    "ng=vec.fit_transform(critics.quote)\n",
    "ng_vectors=ng.toarray()\n",
    "ng_word_freq=ng.toarray().sum(axis=0)\n",
    "print(ng_word_freq)\n",
    "ng_word_doc=Counter(ng_word_freq)\n",
    "ng_word_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_words = list(vec.get_feature_names())\n",
    "ng_word_count = Counter(dict(zip(ng_words, ng_word_freq)))\n",
    "print (ng_word_count.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the min_df: 20 and the alpha: 5 from before with 2 n-grams we get:\n",
      "Accuracy on training data: 0.708289\n",
      "Accuracy on test data:     0.639001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print best of the results        \n",
    "print(\"Using the min_df: {} and the alpha: {} from before with 2 n-grams we get:\".format(best_min_df,best_alpha))\n",
    "\n",
    "\n",
    "# Your turn best_min_df best_alpha\n",
    "vec = CountVectorizer(min_df=best_min_df, ngram_range=(2, 2))\n",
    "\n",
    "x_ng, y_ng = make_xy(critics, vec)\n",
    "\n",
    "xtrainng=x_ng[mask]\n",
    "ytrainng=y_ng[mask]\n",
    "xtestng=x_ng[~mask]\n",
    "ytestng=y_ng[~mask]\n",
    "\n",
    "ngclf = MultinomialNB(alpha= best_alpha).fit(xtrainng, ytrainng)\n",
    "\n",
    "#Print the accuracy on the test and training dataset\n",
    "ngtraining_accuracy = ngclf.score(xtrainng, ytrainng)\n",
    "ngtest_accuracy = ngclf.score(xtestng, ytestng)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(ngtraining_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(ngtest_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like with 2 n-grams we lower the noise in the model but predict worse overall. How should we that this? We miss more predictions but on the correct answers we can be more confident that are model is correct. Lets look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1]), 'fresh: 2-gram')\n",
      "[[ 0.43544347  0.56455653]]\n",
      "(array([0]), 'fresh: word only')\n",
      "[[ 0.55399577  0.44600423]]\n"
     ]
    }
   ],
   "source": [
    "#test review to predict\n",
    "new_text=['This movie is not bad']\n",
    "\n",
    "#2-gram prediction\n",
    "new_vec=vec.transform(new_text)\n",
    "print(ngclf.predict(new_vec),'fresh: 2-gram')\n",
    "print(ngclf.predict_proba(new_vec))\n",
    "\n",
    "#only singel word prediction\n",
    "new_vec=vectorizer.transform(new_text)\n",
    "print(clf.predict(new_vec),'fresh: word only')\n",
    "print(clf.predict_proba(new_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, while not predicting better overall, does do a better job handling the flip pharses when calculating probability. We see that it does at least catch this flip pharse correctly where the original model did not. This is why we wanted to look a the 2 n-gram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the min_df: 20 and the alpha: 5 from before with TF-IDF we get:\n",
      "Accuracy on training data: 0.700150\n",
      "Accuracy on test data:     0.645887\n"
     ]
    }
   ],
   "source": [
    "#print parameters of the results        \n",
    "print(\"Using the min_df: {} and the alpha: {} from before with TF-IDF we get:\".format(best_min_df,best_alpha))\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=best_min_df, stop_words='english')\n",
    "\n",
    "x_tfid, y_tfid = make_xy(critics, tfidfvectorizer)\n",
    "xtraintfid=x_tfid[mask]\n",
    "ytraintfid=y_tfid[mask]\n",
    "xtesttfid=x_tfid[~mask]\n",
    "ytesttfid=y_tfid[~mask]\n",
    "\n",
    "tfid_clf = MultinomialNB(alpha=best_alpha).fit(xtraintfid, ytraintfid)\n",
    "\n",
    "#Print the accuracy on the test and training dataset\n",
    "training_accuracy = tfid_clf.score(xtraintfid, ytraintfid)\n",
    "test_accuracy = tfid_clf.score(xtesttfid, ytesttfid)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accy_score(clf, x, y):\n",
    "    prob = clf.score(x,y)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99614478475048185"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accy_score(rf, xtrainrf, ytrainrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.99\n",
      "Accuracy on test data: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 75%|  | 3/4 [00:00<00:00, 15.47it/s]\u001b[A\n",
      " 25%|       | 1/4 [00:00<00:00,  4.31it/s]\u001b[A\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|       | 1/4 [00:00<00:00,  5.40it/s]\u001b[A\n",
      " 50%|     | 2/4 [00:00<00:00,  3.96it/s]\u001b[A\n",
      " 75%|  | 3/4 [00:01<00:00,  1.54it/s]\u001b[A\n",
      "100%|| 4/4 [00:02<00:00,  1.85it/s]\u001b[A\n",
      " 50%|     | 2/4 [00:02<00:02,  1.20s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|       | 1/4 [00:01<00:05,  1.88s/it]\u001b[A\n",
      " 50%|     | 2/4 [00:04<00:04,  2.42s/it]\u001b[A\n",
      " 75%|  | 3/4 [00:18<00:06,  6.11s/it]\u001b[A\n",
      "100%|| 4/4 [00:20<00:00,  5.13s/it]\u001b[A\n",
      " 75%|  | 3/4 [00:22<00:07,  7.64s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|       | 1/4 [00:18<00:56, 18.84s/it]\u001b[A\n",
      " 50%|     | 2/4 [00:47<00:47, 23.94s/it]\u001b[A\n",
      " 75%|  | 3/4 [03:04<01:01, 61.42s/it]\u001b[A\n",
      "100%|| 4/4 [03:25<00:00, 51.46s/it]\u001b[A\n",
      "100%|| 4/4 [03:48<00:00, 57.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100, 0.99614478475048185)\n",
      "Accuracy on training data with 0: 0.996145\n",
      "Accuracy on test data with 0:     0.687018\n"
     ]
    }
   ],
   "source": [
    "#print best of the results        \n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "x,y=make_xy(critics)\n",
    "\n",
    "# split into training and test sets\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model on the trainng data.\n",
    "rf.fit( x_train,y_train)\n",
    "\n",
    "#print the accuracy on the training data\n",
    "training_accuracy = rf.score(x_train, y_train)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "\n",
    "ftest_accuracy = rf.score(x_test, y_test)\n",
    "print(\"Accuracy on test data: {:0.2f}\".format(ftest_accuracy))\n",
    "\n",
    "\n",
    "# Your turn best_min_df best_alpha\n",
    "vec = CountVectorizer(min_df=best_min_df)\n",
    "\n",
    "x_rf, y_rf = make_xy(critics, vec)\n",
    "\n",
    "xtrainrf=x_rf[mask]\n",
    "ytrainrf=y_rf[mask]\n",
    "xtestrf=x_rf[~mask]\n",
    "ytestrf=y_rf[~mask]\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "best_tree=0\n",
    "#impurity_decrease=[0,.1,.2,.5,.7,1](min_impurity_decrease=i\n",
    "trees=[1,10,100,1000]\n",
    "features=[10,100,1000,'sqrt']\n",
    "best_feature=0\n",
    "best_score=0\n",
    "for i in tqdm(trees):\n",
    "    for j in tqdm(features):\n",
    "        rf=RandomForestClassifier(max_features= j, n_estimators=i)\n",
    "        rf.fit(xtrain,ytrainrf)\n",
    "        \n",
    "        score=accy_score(rf, xtrainrf, ytrainrf)\n",
    "        if score>best_score:\n",
    "            best_score=score\n",
    "            best_tree=i\n",
    "            best_feature=j\n",
    "rf = RandomForestClassifier(max_features= best_feature,n_estimators=best_tree).fit(xtrainrf, ytrainrf)\n",
    "\n",
    "#Print the accuracy on the test and training dataset\n",
    "rftraining_accuracy = rf.score(xtrainrf, ytrainrf)\n",
    "rftest_accuracy = rf.score(xtestrf, ytestrf)\n",
    "print(best_feature,best_tree,best_score)\n",
    "print(\"Accuracy on training data with {}: {:2f}\".format(best_estimators,rftraining_accuracy))\n",
    "print(\"Accuracy on test data with {}:     {:2f}\".format(best_estimators,rftest_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model does even better the 2 n-gram with prediction and noise reduction. However, worse than the original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "model= NMF(n_components=6)\n",
    "model.fit(x_tfid)\n",
    "nfm_features=model.transform(x_tfid)\n",
    "norm=normalize(nfm_features)\n",
    "dfn=pd.DataFrame(norm)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.984958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102567</td>\n",
       "      <td>0.112555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092953</td>\n",
       "      <td>0.880810</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.061723</td>\n",
       "      <td>0.460117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938685</td>\n",
       "      <td>0.344777</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.996790</td>\n",
       "      <td>0.062738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.507318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859614</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916930</td>\n",
       "      <td>0.397324</td>\n",
       "      <td>0.036676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.039935</td>\n",
       "      <td>0.114932</td>\n",
       "      <td>0.981916</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.040790</td>\n",
       "      <td>0.121167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.032244</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.994840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.076522</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.009355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874296</td>\n",
       "      <td>0.443705</td>\n",
       "      <td>0.172027</td>\n",
       "      <td>0.095601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.128051</td>\n",
       "      <td>0.924869</td>\n",
       "      <td>0.098370</td>\n",
       "      <td>0.143444</td>\n",
       "      <td>0.312759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685295</td>\n",
       "      <td>0.644257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339559</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.088852</td>\n",
       "      <td>0.262578</td>\n",
       "      <td>0.735907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617737</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.967961</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.135577</td>\n",
       "      <td>0.191516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987988</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.147962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.139965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662462</td>\n",
       "      <td>0.061203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985538</td>\n",
       "      <td>0.169456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.759170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.984901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165713</td>\n",
       "      <td>0.041767</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>0.018411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.016240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998287</td>\n",
       "      <td>0.046319</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.025105</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.099281</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.986746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.103634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.819375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.944280</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>0.289539</td>\n",
       "      <td>0.153766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15532</th>\n",
       "      <td>0.927819</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.095605</td>\n",
       "      <td>0.054903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15533</th>\n",
       "      <td>0.020225</td>\n",
       "      <td>0.990628</td>\n",
       "      <td>0.123951</td>\n",
       "      <td>0.048291</td>\n",
       "      <td>0.023491</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>0.128361</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.989607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.101096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>0.038295</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.024637</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.143753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.721532</td>\n",
       "      <td>0.519574</td>\n",
       "      <td>0.258803</td>\n",
       "      <td>0.233335</td>\n",
       "      <td>0.255444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.952034</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.291133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.214959</td>\n",
       "      <td>0.918542</td>\n",
       "      <td>0.120081</td>\n",
       "      <td>0.307028</td>\n",
       "      <td>0.032468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15542</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.035474</td>\n",
       "      <td>0.997719</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>0.906164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422675</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15545</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214255</td>\n",
       "      <td>0.824097</td>\n",
       "      <td>0.101856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15546</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.932282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>0.042501</td>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.216476</td>\n",
       "      <td>0.959962</td>\n",
       "      <td>0.059626</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>0.164441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.082563</td>\n",
       "      <td>0.989767</td>\n",
       "      <td>0.030782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>0.183898</td>\n",
       "      <td>0.279381</td>\n",
       "      <td>0.850008</td>\n",
       "      <td>0.099614</td>\n",
       "      <td>0.240902</td>\n",
       "      <td>0.312501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>0.999058</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554</th>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15555</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998672</td>\n",
       "      <td>0.051516</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>0.022905</td>\n",
       "      <td>0.121920</td>\n",
       "      <td>0.667232</td>\n",
       "      <td>0.083171</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>0.728743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545334</td>\n",
       "      <td>0.838219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>0.015703</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.003803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986546</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.162855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15561 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "0      0.073215  0.036170  0.984958  0.000000  0.102567  0.112555\n",
       "1      0.005654  0.000000  0.021850  0.014467  0.000000  0.999641\n",
       "2      0.000000  0.092953  0.880810  0.003863  0.061723  0.460117\n",
       "3      0.000000  0.938685  0.344777  0.000651  0.000000  0.000000\n",
       "4      0.013830  0.017659  0.996790  0.062738  0.000000  0.044396\n",
       "5      0.507318  0.000000  0.859614  0.060765  0.000000  0.000000\n",
       "6      0.000000  0.916930  0.397324  0.036676  0.000000  0.005326\n",
       "7      0.039935  0.114932  0.981916  0.068491  0.040790  0.121167\n",
       "8      0.032244  0.037400  0.994840  0.000000  0.000000  0.088631\n",
       "9      0.005677  0.004269  0.076522  0.996680  0.025193  0.009355\n",
       "10     0.999995  0.000000  0.000000  0.003193  0.000000  0.000000\n",
       "11     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "12     0.000000  0.047599  0.998320  0.000000  0.000000  0.033045\n",
       "13     0.000000  0.000000  0.874296  0.443705  0.172027  0.095601\n",
       "14     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "15     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "16     0.012198  0.128051  0.924869  0.098370  0.143444  0.312759\n",
       "17     0.000000  0.685295  0.644257  0.000000  0.339559  0.001765\n",
       "18     0.088852  0.262578  0.735907  0.000000  0.617737  0.000000\n",
       "19     0.000000  0.089230  0.967961  0.005451  0.135577  0.191516\n",
       "20     0.000000  0.000000  0.987988  0.041833  0.015418  0.147962\n",
       "21     0.008996  0.000000  0.023111  0.000000  0.999692  0.000000\n",
       "22     0.139965  0.000000  0.733354  0.000000  0.662462  0.061203\n",
       "23     0.000000  0.000000  0.985538  0.169456  0.000000  0.000000\n",
       "24     0.006446  0.010077  0.759170  0.000000  0.650783  0.000000\n",
       "25     0.984901  0.000000  0.165713  0.041767  0.020647  0.018411\n",
       "26     0.016240  0.000000  0.998287  0.046319  0.031837  0.000000\n",
       "27     0.001507  0.025105  0.998821  0.041517  0.000000  0.000000\n",
       "28     0.099281  0.075363  0.986746  0.000000  0.007480  0.103634\n",
       "29     0.819375  0.000000  0.573258  0.000000  0.000000  0.000000\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "15531  0.000000  0.027956  0.944280  0.008774  0.289539  0.153766\n",
       "15532  0.927819  0.014126  0.324431  0.146774  0.095605  0.054903\n",
       "15533  0.020225  0.990628  0.123951  0.048291  0.023491  0.000000\n",
       "15534  0.000000  0.000000  0.990991  0.038213  0.128361  0.000000\n",
       "15535  0.000000  0.000000  0.998164  0.000000  0.000000  0.060572\n",
       "15536  0.034250  0.096213  0.989607  0.000000  0.005285  0.101096\n",
       "15537  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "15538  0.038295  0.005587  0.988549  0.024637  0.001131  0.143753\n",
       "15539  0.150859  0.721532  0.519574  0.258803  0.233335  0.255444\n",
       "15540  0.014776  0.092876  0.952034  0.005357  0.291133  0.000000\n",
       "15541  0.018239  0.214959  0.918542  0.120081  0.307028  0.032468\n",
       "15542  0.000000  0.000000  0.999892  0.000000  0.000000  0.014674\n",
       "15543  0.018166  0.035474  0.997719  0.011314  0.053299  0.000000\n",
       "15544  0.906164  0.000000  0.422675  0.014609  0.000000  0.000000\n",
       "15545  0.000000  0.214255  0.824097  0.101856  0.000000  0.514378\n",
       "15546  0.000000  0.900747  0.434343  0.000000  0.000000  0.000000\n",
       "15547  0.107171  0.160349  0.932282  0.000000  0.000000  0.306026\n",
       "15548  0.042501  0.252801  0.959838  0.000000  0.000000  0.114000\n",
       "15549  0.018423  0.216476  0.959962  0.059626  0.026007  0.164441\n",
       "15550  0.001223  0.082563  0.989767  0.030782  0.000000  0.112229\n",
       "15551  0.183898  0.279381  0.850008  0.099614  0.240902  0.312501\n",
       "15552  0.999058  0.040837  0.000000  0.014705  0.000000  0.000000\n",
       "15553  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "15554  0.005346  0.000000  0.999106  0.000000  0.000000  0.041933\n",
       "15555  0.000000  0.998672  0.051516  0.000350  0.000000  0.000000\n",
       "15556  0.022905  0.121920  0.667232  0.083171  0.037789  0.728743\n",
       "15557  0.000000  0.999976  0.000000  0.000000  0.006975  0.000000\n",
       "15558  0.000000  0.545334  0.838219  0.000000  0.000000  0.000000\n",
       "15559  0.006844  0.950462  0.308880  0.015703  0.030127  0.003803\n",
       "15560  0.000000  0.000000  0.986546  0.014358  0.162855  0.000000\n",
       "\n",
       "[15561 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf= TfidfVectorizer()\n",
    "csr_mat = tfidf.fit_transform(critics.quote)\n",
    "words =tfidf.get_feature_names()\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "svd =TruncatedSVD(n_components=50)\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "pipeline=make_pipeline(svd,kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('truncatedsvd', TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)), ('kmeans', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(csr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=pipeline.predict(csr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'labels':labels,'fresh':critics.fresh})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2words=dict((v,k)for k,v in tfidf.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'000',\n",
       " 1: u'0014',\n",
       " 2: u'007',\n",
       " 3: u'044',\n",
       " 4: u'07',\n",
       " 5: u'10',\n",
       " 6: u'100',\n",
       " 7: u'101',\n",
       " 8: u'102',\n",
       " 9: u'104',\n",
       " 10: u'105',\n",
       " 11: u'108',\n",
       " 12: u'10th',\n",
       " 13: u'11',\n",
       " 14: u'110',\n",
       " 15: u'112',\n",
       " 16: u'1138',\n",
       " 17: u'114',\n",
       " 18: u'118',\n",
       " 19: u'11th',\n",
       " 20: u'12',\n",
       " 21: u'124',\n",
       " 22: u'125',\n",
       " 23: u'128',\n",
       " 24: u'13',\n",
       " 25: u'130',\n",
       " 26: u'131',\n",
       " 27: u'132',\n",
       " 28: u'133',\n",
       " 29: u'134',\n",
       " 30: u'136',\n",
       " 31: u'137',\n",
       " 32: u'13th',\n",
       " 33: u'14',\n",
       " 34: u'141',\n",
       " 35: u'145',\n",
       " 36: u'15',\n",
       " 37: u'153',\n",
       " 38: u'15th',\n",
       " 39: u'16',\n",
       " 40: u'160',\n",
       " 41: u'161',\n",
       " 42: u'1660s',\n",
       " 43: u'16mm',\n",
       " 44: u'16th',\n",
       " 45: u'17',\n",
       " 46: u'17th',\n",
       " 47: u'18',\n",
       " 48: u'180',\n",
       " 49: u'185',\n",
       " 50: u'1850',\n",
       " 51: u'187',\n",
       " 52: u'1873',\n",
       " 53: u'1879',\n",
       " 54: u'1880s',\n",
       " 55: u'1898',\n",
       " 56: u'18a',\n",
       " 57: u'18th',\n",
       " 58: u'19',\n",
       " 59: u'1900s',\n",
       " 60: u'1910',\n",
       " 61: u'1911',\n",
       " 62: u'1914',\n",
       " 63: u'1917',\n",
       " 64: u'1920',\n",
       " 65: u'1920s',\n",
       " 66: u'1921',\n",
       " 67: u'1922',\n",
       " 68: u'1925',\n",
       " 69: u'1926',\n",
       " 70: u'1927',\n",
       " 71: u'1930',\n",
       " 72: u'1930s',\n",
       " 73: u'1931',\n",
       " 74: u'1932',\n",
       " 75: u'1933',\n",
       " 76: u'1934',\n",
       " 77: u'1935',\n",
       " 78: u'1936',\n",
       " 79: u'1937',\n",
       " 80: u'1938',\n",
       " 81: u'1939',\n",
       " 82: u'1940',\n",
       " 83: u'1940s',\n",
       " 84: u'1941',\n",
       " 85: u'1942',\n",
       " 86: u'1943',\n",
       " 87: u'1946',\n",
       " 88: u'1948',\n",
       " 89: u'1950',\n",
       " 90: u'1950s',\n",
       " 91: u'1951',\n",
       " 92: u'1952',\n",
       " 93: u'1953',\n",
       " 94: u'1954',\n",
       " 95: u'1955',\n",
       " 96: u'1956',\n",
       " 97: u'1957',\n",
       " 98: u'1958',\n",
       " 99: u'1959',\n",
       " 100: u'1960',\n",
       " 101: u'1960s',\n",
       " 102: u'1961',\n",
       " 103: u'1962',\n",
       " 104: u'1963',\n",
       " 105: u'1964',\n",
       " 106: u'1965',\n",
       " 107: u'1966',\n",
       " 108: u'1967',\n",
       " 109: u'1968',\n",
       " 110: u'1969',\n",
       " 111: u'1970',\n",
       " 112: u'1970s',\n",
       " 113: u'1971',\n",
       " 114: u'1972',\n",
       " 115: u'1973',\n",
       " 116: u'1974',\n",
       " 117: u'1975',\n",
       " 118: u'1976',\n",
       " 119: u'1977',\n",
       " 120: u'1978',\n",
       " 121: u'1979',\n",
       " 122: u'1980',\n",
       " 123: u'1980s',\n",
       " 124: u'1981',\n",
       " 125: u'1982',\n",
       " 126: u'1983',\n",
       " 127: u'1984',\n",
       " 128: u'1985',\n",
       " 129: u'1986',\n",
       " 130: u'1987',\n",
       " 131: u'1988',\n",
       " 132: u'1989',\n",
       " 133: u'1990',\n",
       " 134: u'1990s',\n",
       " 135: u'1991',\n",
       " 136: u'1992',\n",
       " 137: u'1993',\n",
       " 138: u'1994',\n",
       " 139: u'1995',\n",
       " 140: u'1996',\n",
       " 141: u'1997',\n",
       " 142: u'1998',\n",
       " 143: u'1999',\n",
       " 144: u'19th',\n",
       " 145: u'20',\n",
       " 146: u'200',\n",
       " 147: u'2000',\n",
       " 148: u'20000',\n",
       " 149: u'2001',\n",
       " 150: u'2002',\n",
       " 151: u'2003',\n",
       " 152: u'2004',\n",
       " 153: u'2006',\n",
       " 154: u'2007',\n",
       " 155: u'2008',\n",
       " 156: u'2009',\n",
       " 157: u'2010',\n",
       " 158: u'2019',\n",
       " 159: u'202',\n",
       " 160: u'2033',\n",
       " 161: u'207',\n",
       " 162: u'208',\n",
       " 163: u'20s',\n",
       " 164: u'20th',\n",
       " 165: u'21st',\n",
       " 166: u'22',\n",
       " 167: u'231',\n",
       " 168: u'24',\n",
       " 169: u'240',\n",
       " 170: u'25',\n",
       " 171: u'250',\n",
       " 172: u'25th',\n",
       " 173: u'26',\n",
       " 174: u'27',\n",
       " 175: u'28',\n",
       " 176: u'29',\n",
       " 177: u'2d',\n",
       " 178: u'30',\n",
       " 179: u'300',\n",
       " 180: u'3000',\n",
       " 181: u'30s',\n",
       " 182: u'31',\n",
       " 183: u'312',\n",
       " 184: u'33',\n",
       " 185: u'33rd',\n",
       " 186: u'34',\n",
       " 187: u'34th',\n",
       " 188: u'35',\n",
       " 189: u'35mm',\n",
       " 190: u'36',\n",
       " 191: u'37',\n",
       " 192: u'39',\n",
       " 193: u'3d',\n",
       " 194: u'40',\n",
       " 195: u'400',\n",
       " 196: u'40s',\n",
       " 197: u'41',\n",
       " 198: u'42',\n",
       " 199: u'42nd',\n",
       " 200: u'45',\n",
       " 201: u'467',\n",
       " 202: u'48',\n",
       " 203: u'48hrs',\n",
       " 204: u'50',\n",
       " 205: u'500',\n",
       " 206: u'50s',\n",
       " 207: u'51',\n",
       " 208: u'52',\n",
       " 209: u'54',\n",
       " 210: u'55',\n",
       " 211: u'56',\n",
       " 212: u'60',\n",
       " 213: u'60s',\n",
       " 214: u'63',\n",
       " 215: u'64257',\n",
       " 216: u'65',\n",
       " 217: u'666',\n",
       " 218: u'69',\n",
       " 219: u'70',\n",
       " 220: u'70s',\n",
       " 221: u'72',\n",
       " 222: u'73',\n",
       " 223: u'74',\n",
       " 224: u'747',\n",
       " 225: u'747s',\n",
       " 226: u'75',\n",
       " 227: u'75th',\n",
       " 228: u'76',\n",
       " 229: u'78',\n",
       " 230: u'79',\n",
       " 231: u'80',\n",
       " 232: u'80s',\n",
       " 233: u'81',\n",
       " 234: u'82',\n",
       " 235: u'8212',\n",
       " 236: u'83',\n",
       " 237: u'84',\n",
       " 238: u'85',\n",
       " 239: u'87',\n",
       " 240: u'88',\n",
       " 241: u'90',\n",
       " 242: u'90s',\n",
       " 243: u'91',\n",
       " 244: u'92',\n",
       " 245: u'93',\n",
       " 246: u'94',\n",
       " 247: u'95',\n",
       " 248: u'96',\n",
       " 249: u'97',\n",
       " 250: u'98',\n",
       " 251: u'99',\n",
       " 252: u'_and_',\n",
       " 253: u'aaron',\n",
       " 254: u'abandon',\n",
       " 255: u'abandoned',\n",
       " 256: u'abandonment',\n",
       " 257: u'abandons',\n",
       " 258: u'abbott',\n",
       " 259: u'abbreviated',\n",
       " 260: u'abc',\n",
       " 261: u'abdominal',\n",
       " 262: u'abduct',\n",
       " 263: u'abe',\n",
       " 264: u'abel',\n",
       " 265: u'abets',\n",
       " 266: u'abiding',\n",
       " 267: u'abilities',\n",
       " 268: u'ability',\n",
       " 269: u'abject',\n",
       " 270: u'ablazin',\n",
       " 271: u'able',\n",
       " 272: u'abler',\n",
       " 273: u'ably',\n",
       " 274: u'aboard',\n",
       " 275: u'abominable',\n",
       " 276: u'abomination',\n",
       " 277: u'aborted',\n",
       " 278: u'abortion',\n",
       " 279: u'abounce',\n",
       " 280: u'abound',\n",
       " 281: u'abounds',\n",
       " 282: u'about',\n",
       " 283: u'above',\n",
       " 284: u'abrams',\n",
       " 285: u'abrasion',\n",
       " 286: u'abrasive',\n",
       " 287: u'abroad',\n",
       " 288: u'abrupt',\n",
       " 289: u'abruptly',\n",
       " 290: u'absence',\n",
       " 291: u'absent',\n",
       " 292: u'absolute',\n",
       " 293: u'absolutely',\n",
       " 294: u'absorbed',\n",
       " 295: u'absorbing',\n",
       " 296: u'absorbingly',\n",
       " 297: u'absorption',\n",
       " 298: u'abstinence',\n",
       " 299: u'abstract',\n",
       " 300: u'abstraction',\n",
       " 301: u'abstractions',\n",
       " 302: u'absurd',\n",
       " 303: u'absurdism',\n",
       " 304: u'absurdist',\n",
       " 305: u'absurdities',\n",
       " 306: u'absurdity',\n",
       " 307: u'absurdly',\n",
       " 308: u'absurdum',\n",
       " 309: u'abundance',\n",
       " 310: u'abundant',\n",
       " 311: u'abundantly',\n",
       " 312: u'abuse',\n",
       " 313: u'abused',\n",
       " 314: u'abusing',\n",
       " 315: u'abysmal',\n",
       " 316: u'abysmally',\n",
       " 317: u'abyss',\n",
       " 318: u'academic',\n",
       " 319: u'academically',\n",
       " 320: u'academy',\n",
       " 321: u'accelerates',\n",
       " 322: u'accelerating',\n",
       " 323: u'accent',\n",
       " 324: u'accents',\n",
       " 325: u'accentuate',\n",
       " 326: u'accentuated',\n",
       " 327: u'accept',\n",
       " 328: u'acceptable',\n",
       " 329: u'acceptance',\n",
       " 330: u'accepted',\n",
       " 331: u'accepts',\n",
       " 332: u'accessible',\n",
       " 333: u'accident',\n",
       " 334: u'accidental',\n",
       " 335: u'accidential',\n",
       " 336: u'acclaim',\n",
       " 337: u'acclaimed',\n",
       " 338: u'accommodation',\n",
       " 339: u'accompanied',\n",
       " 340: u'accompanies',\n",
       " 341: u'accompany',\n",
       " 342: u'accompanying',\n",
       " 343: u'accomplices',\n",
       " 344: u'accomplish',\n",
       " 345: u'accomplished',\n",
       " 346: u'accomplishes',\n",
       " 347: u'accomplishment',\n",
       " 348: u'accomplishments',\n",
       " 349: u'accorded',\n",
       " 350: u'according',\n",
       " 351: u'accordion',\n",
       " 352: u'account',\n",
       " 353: u'accounting',\n",
       " 354: u'accounts',\n",
       " 355: u'accouterments',\n",
       " 356: u'accrues',\n",
       " 357: u'accumulated',\n",
       " 358: u'accumulates',\n",
       " 359: u'accumulating',\n",
       " 360: u'accumulation',\n",
       " 361: u'accuracy',\n",
       " 362: u'accurate',\n",
       " 363: u'accurately',\n",
       " 364: u'accusations',\n",
       " 365: u'accusatory',\n",
       " 366: u'accused',\n",
       " 367: u'accustomed',\n",
       " 368: u'ace',\n",
       " 369: u'acerbic',\n",
       " 370: u'aces',\n",
       " 371: u'ache',\n",
       " 372: u'achieve',\n",
       " 373: u'achieved',\n",
       " 374: u'achievement',\n",
       " 375: u'achievements',\n",
       " 376: u'achieves',\n",
       " 377: u'achieving',\n",
       " 378: u'achilles',\n",
       " 379: u'aching',\n",
       " 380: u'achingly',\n",
       " 381: u'acid',\n",
       " 382: u'ackerman',\n",
       " 383: u'acknowledge',\n",
       " 384: u'acknowledged',\n",
       " 385: u'acknowledgment',\n",
       " 386: u'acne',\n",
       " 387: u'acolytes',\n",
       " 388: u'acquaintance',\n",
       " 389: u'acquainted',\n",
       " 390: u'acquire',\n",
       " 391: u'acquired',\n",
       " 392: u'acquires',\n",
       " 393: u'acquit',\n",
       " 394: u'acquits',\n",
       " 395: u'acrid',\n",
       " 396: u'acrimonious',\n",
       " 397: u'acrobatics',\n",
       " 398: u'acrophobia',\n",
       " 399: u'across',\n",
       " 400: u'act',\n",
       " 401: u'acted',\n",
       " 402: u'acting',\n",
       " 403: u'action',\n",
       " 404: u'actioner',\n",
       " 405: u'actioners',\n",
       " 406: u'actionful',\n",
       " 407: u'actions',\n",
       " 408: u'active',\n",
       " 409: u'actively',\n",
       " 410: u'activists',\n",
       " 411: u'activities',\n",
       " 412: u'activity',\n",
       " 413: u'actor',\n",
       " 414: u'actors',\n",
       " 415: u'actress',\n",
       " 416: u'actresses',\n",
       " 417: u'acts',\n",
       " 418: u'actual',\n",
       " 419: u'actuality',\n",
       " 420: u'actually',\n",
       " 421: u'acuity',\n",
       " 422: u'acute',\n",
       " 423: u'acutely',\n",
       " 424: u'ad',\n",
       " 425: u'adage',\n",
       " 426: u'adam',\n",
       " 427: u'adams',\n",
       " 428: u'adapation',\n",
       " 429: u'adapt',\n",
       " 430: u'adaptable',\n",
       " 431: u'adaptation',\n",
       " 432: u'adaptations',\n",
       " 433: u'adapted',\n",
       " 434: u'adapters',\n",
       " 435: u'adapting',\n",
       " 436: u'adaption',\n",
       " 437: u'adapts',\n",
       " 438: u'add',\n",
       " 439: u'addams',\n",
       " 440: u'addamses',\n",
       " 441: u'added',\n",
       " 442: u'addiction',\n",
       " 443: u'addictive',\n",
       " 444: u'addicts',\n",
       " 445: u'adding',\n",
       " 446: u'addition',\n",
       " 447: u'additional',\n",
       " 448: u'additions',\n",
       " 449: u'addled',\n",
       " 450: u'address',\n",
       " 451: u'addresses',\n",
       " 452: u'addressing',\n",
       " 453: u'adds',\n",
       " 454: u'adele',\n",
       " 455: u'adept',\n",
       " 456: u'adeptly',\n",
       " 457: u'adeptness',\n",
       " 458: u'adequate',\n",
       " 459: u'adequately',\n",
       " 460: u'adhere',\n",
       " 461: u'adherence',\n",
       " 462: u'adheres',\n",
       " 463: u'adieu',\n",
       " 464: u'adjective',\n",
       " 465: u'adjectives',\n",
       " 466: u'adjoining',\n",
       " 467: u'adjust',\n",
       " 468: u'adjuster',\n",
       " 469: u'adler',\n",
       " 470: u'administration',\n",
       " 471: u'admirable',\n",
       " 472: u'admirably',\n",
       " 473: u'admiration',\n",
       " 474: u'admire',\n",
       " 475: u'admired',\n",
       " 476: u'admirer',\n",
       " 477: u'admirers',\n",
       " 478: u'admires',\n",
       " 479: u'admiring',\n",
       " 480: u'admiringly',\n",
       " 481: u'admission',\n",
       " 482: u'admit',\n",
       " 483: u'admitted',\n",
       " 484: u'admittedly',\n",
       " 485: u'adolescence',\n",
       " 486: u'adolescent',\n",
       " 487: u'adolescents',\n",
       " 488: u'adolf',\n",
       " 489: u'adolph',\n",
       " 490: u'adopt',\n",
       " 491: u'adopted',\n",
       " 492: u'adopting',\n",
       " 493: u'adoption',\n",
       " 494: u'adorable',\n",
       " 495: u'adorableness',\n",
       " 496: u'adoration',\n",
       " 497: u'adored',\n",
       " 498: u'adores',\n",
       " 499: u'adoring',\n",
       " 500: u'adorning',\n",
       " 501: u'adrenaline',\n",
       " 502: u'adrenalized',\n",
       " 503: u'adrian',\n",
       " 504: u'adrienne',\n",
       " 505: u'adrift',\n",
       " 506: u'adroit',\n",
       " 507: u'adroitly',\n",
       " 508: u'ads',\n",
       " 509: u'adulation',\n",
       " 510: u'adult',\n",
       " 511: u'adulterous',\n",
       " 512: u'adultery',\n",
       " 513: u'adulthood',\n",
       " 514: u'adults',\n",
       " 515: u'advance',\n",
       " 516: u'advanced',\n",
       " 517: u'advancements',\n",
       " 518: u'advances',\n",
       " 519: u'advancing',\n",
       " 520: u'advantage',\n",
       " 521: u'adventure',\n",
       " 522: u'adventurer',\n",
       " 523: u'adventures',\n",
       " 524: u'adventurism',\n",
       " 525: u'adventurous',\n",
       " 526: u'adversaries',\n",
       " 527: u'advert',\n",
       " 528: u'advertised',\n",
       " 529: u'advertises',\n",
       " 530: u'advertising',\n",
       " 531: u'advice',\n",
       " 532: u'advised',\n",
       " 533: u'advising',\n",
       " 534: u'advocate',\n",
       " 535: u'aerial',\n",
       " 536: u'aerobic',\n",
       " 537: u'aeronautically',\n",
       " 538: u'aerosol',\n",
       " 539: u'aeschylus',\n",
       " 540: u'aesthetic',\n",
       " 541: u'aesthetically',\n",
       " 542: u'aestheticism',\n",
       " 543: u'aestheticization',\n",
       " 544: u'aestheticized',\n",
       " 545: u'afar',\n",
       " 546: u'affability',\n",
       " 547: u'affable',\n",
       " 548: u'affair',\n",
       " 549: u'affairs',\n",
       " 550: u'affect',\n",
       " 551: u'affectation',\n",
       " 552: u'affectations',\n",
       " 553: u'affected',\n",
       " 554: u'affecting',\n",
       " 555: u'affectingly',\n",
       " 556: u'affection',\n",
       " 557: u'affectionate',\n",
       " 558: u'affectionately',\n",
       " 559: u'affections',\n",
       " 560: u'affective',\n",
       " 561: u'affectless',\n",
       " 562: u'affectlessness',\n",
       " 563: u'affects',\n",
       " 564: u'affirmation',\n",
       " 565: u'affirming',\n",
       " 566: u'affirms',\n",
       " 567: u'affixed',\n",
       " 568: u'affleck',\n",
       " 569: u'afflict',\n",
       " 570: u'affliction',\n",
       " 571: u'afflicts',\n",
       " 572: u'afford',\n",
       " 573: u'affords',\n",
       " 574: u'afi',\n",
       " 575: u'aficionado',\n",
       " 576: u'aficionados',\n",
       " 577: u'afloat',\n",
       " 578: u'aforementioned',\n",
       " 579: u'aforethought',\n",
       " 580: u'afraid',\n",
       " 581: u'afresh',\n",
       " 582: u'africa',\n",
       " 583: u'african',\n",
       " 584: u'after',\n",
       " 585: u'aftereffect',\n",
       " 586: u'afterlife',\n",
       " 587: u'aftermath',\n",
       " 588: u'afternoon',\n",
       " 589: u'aftertaste',\n",
       " 590: u'afterthought',\n",
       " 591: u'afterward',\n",
       " 592: u'afterwards',\n",
       " 593: u'again',\n",
       " 594: u'against',\n",
       " 595: u'age',\n",
       " 596: u'aged',\n",
       " 597: u'agee',\n",
       " 598: u'ageless',\n",
       " 599: u'agencies',\n",
       " 600: u'agenda',\n",
       " 601: u'agendas',\n",
       " 602: u'agent',\n",
       " 603: u'agents',\n",
       " 604: u'ager',\n",
       " 605: u'agers',\n",
       " 606: u'ages',\n",
       " 607: u'aggregation',\n",
       " 608: u'aggressive',\n",
       " 609: u'aggressively',\n",
       " 610: u'aggrieved',\n",
       " 611: u'aghast',\n",
       " 612: u'agile',\n",
       " 613: u'aging',\n",
       " 614: u'agitprop',\n",
       " 615: u'agnes',\n",
       " 616: u'agnieszka',\n",
       " 617: u'ago',\n",
       " 618: u'agonies',\n",
       " 619: u'agonized',\n",
       " 620: u'agonizing',\n",
       " 621: u'agony',\n",
       " 622: u'agree',\n",
       " 623: u'agreeable',\n",
       " 624: u'agreeably',\n",
       " 625: u'agreed',\n",
       " 626: u'agreement',\n",
       " 627: u'agrees',\n",
       " 628: u'agripping',\n",
       " 629: u'aground',\n",
       " 630: u'aguirre',\n",
       " 631: u'ah',\n",
       " 632: u'ahead',\n",
       " 633: u'ahhhs',\n",
       " 634: u'ahhing',\n",
       " 635: u'aid',\n",
       " 636: u'aide',\n",
       " 637: u'aided',\n",
       " 638: u'aids',\n",
       " 639: u'aim',\n",
       " 640: u'aimed',\n",
       " 641: u'aimee',\n",
       " 642: u'aiming',\n",
       " 643: u'aimless',\n",
       " 644: u'aimlessness',\n",
       " 645: u'aims',\n",
       " 646: u'ain',\n",
       " 647: u'air',\n",
       " 648: u'airborne',\n",
       " 649: u'airheaded',\n",
       " 650: u'airheads',\n",
       " 651: u'airier',\n",
       " 652: u'airiness',\n",
       " 653: u'airless',\n",
       " 654: u'airness',\n",
       " 655: u'airplane',\n",
       " 656: u'airport',\n",
       " 657: u'airports',\n",
       " 658: u'airy',\n",
       " 659: u'aisle',\n",
       " 660: u'aisles',\n",
       " 661: u'aka',\n",
       " 662: u'akerman',\n",
       " 663: u'akin',\n",
       " 664: u'akira',\n",
       " 665: u'akiva',\n",
       " 666: u'al',\n",
       " 667: u'alabama',\n",
       " 668: u'aladdin',\n",
       " 669: u'alan',\n",
       " 670: u'alanis',\n",
       " 671: u'alarm',\n",
       " 672: u'alarmed',\n",
       " 673: u'alarming',\n",
       " 674: u'alarmingly',\n",
       " 675: u'alarmist',\n",
       " 676: u'alarms',\n",
       " 677: u'alas',\n",
       " 678: u'alaska',\n",
       " 679: u'alaskan',\n",
       " 680: u'albatross',\n",
       " 681: u'albee',\n",
       " 682: u'albeit',\n",
       " 683: u'albert',\n",
       " 684: u'album',\n",
       " 685: u'alcatraz',\n",
       " 686: u'alchemist',\n",
       " 687: u'alchemizes',\n",
       " 688: u'alchemy',\n",
       " 689: u'alcoholics',\n",
       " 690: u'alcoholism',\n",
       " 691: u'aldrich',\n",
       " 692: u'alec',\n",
       " 693: u'alecky',\n",
       " 694: u'alert',\n",
       " 695: u'alertly',\n",
       " 696: u'alex',\n",
       " 697: u'alexander',\n",
       " 698: u'alfonso',\n",
       " 699: u'alfred',\n",
       " 700: u'algar',\n",
       " 701: u'algebra',\n",
       " 702: u'algiers',\n",
       " 703: u'algonquin',\n",
       " 704: u'ali',\n",
       " 705: u'alias',\n",
       " 706: u'alibi',\n",
       " 707: u'alibis',\n",
       " 708: u'alice',\n",
       " 709: u'alien',\n",
       " 710: u'alienated',\n",
       " 711: u'alienates',\n",
       " 712: u'alienating',\n",
       " 713: u'alienation',\n",
       " 714: u'aliens',\n",
       " 715: u'alientaion',\n",
       " 716: u'alike',\n",
       " 717: u'alive',\n",
       " 718: u'alkalai',\n",
       " 719: u'all',\n",
       " 720: u'alleged',\n",
       " 721: u'allegedly',\n",
       " 722: u'alleges',\n",
       " 723: u'allegorical',\n",
       " 724: u'allegory',\n",
       " 725: u'allen',\n",
       " 726: u'allende',\n",
       " 727: u'alley',\n",
       " 728: u'alliance',\n",
       " 729: u'alliances',\n",
       " 730: u'allied',\n",
       " 731: u'alligator',\n",
       " 732: u'allow',\n",
       " 733: u'allowance',\n",
       " 734: u'allowed',\n",
       " 735: u'allowing',\n",
       " 736: u'allows',\n",
       " 737: u'allright',\n",
       " 738: u'allude',\n",
       " 739: u'alluding',\n",
       " 740: u'allure',\n",
       " 741: u'alluring',\n",
       " 742: u'allusions',\n",
       " 743: u'allusive',\n",
       " 744: u'almodovar',\n",
       " 745: u'almodovariety',\n",
       " 746: u'almost',\n",
       " 747: u'alone',\n",
       " 748: u'along',\n",
       " 749: u'alongside',\n",
       " 750: u'aloof',\n",
       " 751: u'aloofness',\n",
       " 752: u'aloud',\n",
       " 753: u'alphabet',\n",
       " 754: u'alphabetical',\n",
       " 755: u'already',\n",
       " 756: u'alright',\n",
       " 757: u'also',\n",
       " 758: u'altar',\n",
       " 759: u'alter',\n",
       " 760: u'alteration',\n",
       " 761: u'altered',\n",
       " 762: u'altering',\n",
       " 763: u'alternate',\n",
       " 764: u'alternately',\n",
       " 765: u'alternates',\n",
       " 766: u'alternating',\n",
       " 767: u'alternations',\n",
       " 768: u'alternative',\n",
       " 769: u'althea',\n",
       " 770: u'although',\n",
       " 771: u'altitude',\n",
       " 772: u'altman',\n",
       " 773: u'altogether',\n",
       " 774: u'altruism',\n",
       " 775: u'alvin',\n",
       " 776: u'always',\n",
       " 777: u'alyssa',\n",
       " 778: u'am',\n",
       " 779: u'amadeus',\n",
       " 780: u'amalgam',\n",
       " 781: u'amalgamates',\n",
       " 782: u'amalgamation',\n",
       " 783: u'amalgams',\n",
       " 784: u'amalric',\n",
       " 785: u'amanda',\n",
       " 786: u'amarcord',\n",
       " 787: u'amateur',\n",
       " 788: u'amateurish',\n",
       " 789: u'amateurishly',\n",
       " 790: u'amateurs',\n",
       " 791: u'amaze',\n",
       " 792: u'amazed',\n",
       " 793: u'amazement',\n",
       " 794: u'amazements',\n",
       " 795: u'amazes',\n",
       " 796: u'amazing',\n",
       " 797: u'amazingly',\n",
       " 798: u'amazon',\n",
       " 799: u'ambiance',\n",
       " 800: u'ambience',\n",
       " 801: u'ambient',\n",
       " 802: u'ambiguities',\n",
       " 803: u'ambiguity',\n",
       " 804: u'ambiguous',\n",
       " 805: u'ambiguously',\n",
       " 806: u'ambition',\n",
       " 807: u'ambitions',\n",
       " 808: u'ambitious',\n",
       " 809: u'ambivalence',\n",
       " 810: u'ambivalent',\n",
       " 811: u'amble',\n",
       " 812: u'ambles',\n",
       " 813: u'amblin',\n",
       " 814: u'ambush',\n",
       " 815: u'ameche',\n",
       " 816: u'amenabar',\n",
       " 817: u'amendment',\n",
       " 818: u'amends',\n",
       " 819: u'america',\n",
       " 820: u'american',\n",
       " 821: u'americana',\n",
       " 822: u'americans',\n",
       " 823: u'ames',\n",
       " 824: u'amiability',\n",
       " 825: u'amiable',\n",
       " 826: u'amiably',\n",
       " 827: u'amicus',\n",
       " 828: u'amid',\n",
       " 829: u'amidst',\n",
       " 830: u'amigos',\n",
       " 831: u'amin',\n",
       " 832: u'amiss',\n",
       " 833: u'amistad',\n",
       " 834: u'amityville',\n",
       " 835: u'amma',\n",
       " 836: u'ammunition',\n",
       " 837: u'amnesia',\n",
       " 838: u'amok',\n",
       " 839: u'among',\n",
       " 840: u'amoral',\n",
       " 841: u'amorality',\n",
       " 842: u'amore',\n",
       " 843: u'amount',\n",
       " 844: u'amounting',\n",
       " 845: u'amounts',\n",
       " 846: u'amour',\n",
       " 847: u'amp',\n",
       " 848: u'amped',\n",
       " 849: u'amphetamines',\n",
       " 850: u'amphibian',\n",
       " 851: u'ample',\n",
       " 852: u'amplified',\n",
       " 853: u'amplifier',\n",
       " 854: u'amplify',\n",
       " 855: u'amply',\n",
       " 856: u'amuse',\n",
       " 857: u'amused',\n",
       " 858: u'amusement',\n",
       " 859: u'amusing',\n",
       " 860: u'amusingly',\n",
       " 861: u'amy',\n",
       " 862: u'amyotrophic',\n",
       " 863: u'an',\n",
       " 864: u'anachronistic',\n",
       " 865: u'anachronistically',\n",
       " 866: u'anaconda',\n",
       " 867: u'anaemic',\n",
       " 868: u'analog',\n",
       " 869: u'analogies',\n",
       " 870: u'analogy',\n",
       " 871: u'analysis',\n",
       " 872: u'analytic',\n",
       " 873: u'analytical',\n",
       " 874: u'anamolously',\n",
       " 875: u'anarchic',\n",
       " 876: u'anarchical',\n",
       " 877: u'anarchies',\n",
       " 878: u'anarchist',\n",
       " 879: u'anarchistic',\n",
       " 880: u'anarchy',\n",
       " 881: u'anarene',\n",
       " 882: u'anastasia',\n",
       " 883: u'anatole',\n",
       " 884: u'anatomy',\n",
       " 885: u'ancestors',\n",
       " 886: u'anchor',\n",
       " 887: u'anchored',\n",
       " 888: u'anchoring',\n",
       " 889: u'anchors',\n",
       " 890: u'ancient',\n",
       " 891: u'and',\n",
       " 892: u'anders',\n",
       " 893: u'andersen',\n",
       " 894: u'anderson',\n",
       " 895: u'andiswa',\n",
       " 896: u'andre',\n",
       " 897: u'andrea',\n",
       " 898: u'andrei',\n",
       " 899: u'andress',\n",
       " 900: u'andrew',\n",
       " 901: u'andrews',\n",
       " 902: u'androgynous',\n",
       " 903: u'androgynously',\n",
       " 904: u'android',\n",
       " 905: u'androids',\n",
       " 906: u'andronicus',\n",
       " 907: u'andrus',\n",
       " 908: u'andrzej',\n",
       " 909: u'andy',\n",
       " 910: u'anecdotal',\n",
       " 911: u'anecdote',\n",
       " 912: u'anecdotes',\n",
       " 913: u'anemic',\n",
       " 914: u'anesthesia',\n",
       " 915: u'anew',\n",
       " 916: u'ang',\n",
       " 917: u'angel',\n",
       " 918: u'angela',\n",
       " 919: u'angeleno',\n",
       " 920: u'angeles',\n",
       " 921: u'angelic',\n",
       " 922: u'angelina',\n",
       " 923: u'angelo',\n",
       " 924: u'angelopoulos',\n",
       " 925: u'angels',\n",
       " 926: u'anger',\n",
       " 927: u'angle',\n",
       " 928: u'angled',\n",
       " 929: u'angles',\n",
       " 930: u'anglo',\n",
       " 931: u'angora',\n",
       " 932: u'angry',\n",
       " 933: u'angst',\n",
       " 934: u'anguish',\n",
       " 935: u'anguished',\n",
       " 936: u'angular',\n",
       " 937: u'animal',\n",
       " 938: u'animals',\n",
       " 939: u'animated',\n",
       " 940: u'animates',\n",
       " 941: u'animating',\n",
       " 942: u'animation',\n",
       " 943: u'animator',\n",
       " 944: u'animators',\n",
       " 945: u'animatronic',\n",
       " 946: u'animatronics',\n",
       " 947: u'anime',\n",
       " 948: u'animism',\n",
       " 949: u'animistic',\n",
       " 950: u'aniston',\n",
       " 951: u'anjelica',\n",
       " 952: u'ankle',\n",
       " 953: u'ankles',\n",
       " 954: u'ann',\n",
       " 955: u'anna',\n",
       " 956: u'annals',\n",
       " 957: u'anne',\n",
       " 958: u'annie',\n",
       " 959: u'annihilating',\n",
       " 960: u'anniversary',\n",
       " 961: u'announce',\n",
       " 962: u'annoy',\n",
       " 963: u'annoyance',\n",
       " 964: u'annoying',\n",
       " 965: u'annoyingly',\n",
       " 966: u'annual',\n",
       " 967: u'annually',\n",
       " 968: u'anomalous',\n",
       " 969: u'anomaly',\n",
       " 970: u'anonymous',\n",
       " 971: u'another',\n",
       " 972: u'anouk',\n",
       " 973: u'anspaugh',\n",
       " 974: u'answer',\n",
       " 975: u'answers',\n",
       " 976: u'ant',\n",
       " 977: u'antagonists',\n",
       " 978: u'ante',\n",
       " 979: u'anthem',\n",
       " 980: u'anthems',\n",
       " 981: u'anthology',\n",
       " 982: u'anthony',\n",
       " 983: u'anthropomorphic',\n",
       " 984: u'anthropomorphism',\n",
       " 985: u'anti',\n",
       " 986: u'antiadult',\n",
       " 987: u'antic',\n",
       " 988: u'anticipate',\n",
       " 989: u'anticipated',\n",
       " 990: u'anticipates',\n",
       " 991: u'anticipating',\n",
       " 992: u'anticipation',\n",
       " 993: u'anticlerical',\n",
       " 994: u'anticlimactic',\n",
       " 995: u'anticlimax',\n",
       " 996: u'antics',\n",
       " 997: u'antidote',\n",
       " 998: u'antipollution',\n",
       " 999: u'antiquated',\n",
       " ...}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        fresh  labels\n",
      "1       fresh       0\n",
      "12163  rotten       0\n",
      "23140   fresh       0\n",
      "12165  rotten       0\n",
      "12166  rotten       0\n",
      "12168   fresh       0\n",
      "23139   fresh       0\n",
      "12170   fresh       0\n",
      "12171  rotten       0\n",
      "12172  rotten       0\n",
      "12174  rotten       0\n",
      "12200   fresh       0\n",
      "12204  rotten       0\n",
      "12205  rotten       0\n",
      "12206  rotten       0\n",
      "12207  rotten       0\n",
      "12162   fresh       0\n",
      "12160   fresh       0\n",
      "23141   fresh       0\n",
      "12158  rotten       0\n",
      "12133  rotten       0\n",
      "23148   fresh       0\n",
      "12142   fresh       0\n",
      "12143  rotten       0\n",
      "12144   fresh       0\n",
      "12145   fresh       0\n",
      "23147   fresh       0\n",
      "23137   fresh       0\n",
      "12147   fresh       0\n",
      "12149   fresh       0\n",
      "...       ...     ...\n",
      "13133  rotten       1\n",
      "13135   fresh       1\n",
      "13085  rotten       1\n",
      "13136  rotten       1\n",
      "13148   fresh       1\n",
      "13149   fresh       1\n",
      "13151  rotten       1\n",
      "13153   fresh       1\n",
      "13161  rotten       1\n",
      "13173   fresh       1\n",
      "13139   fresh       1\n",
      "12996   fresh       1\n",
      "13078   fresh       1\n",
      "13038   fresh       1\n",
      "13001  rotten       1\n",
      "13003   fresh       1\n",
      "13005   fresh       1\n",
      "13006   fresh       1\n",
      "13017   fresh       1\n",
      "13021   fresh       1\n",
      "13076  rotten       1\n",
      "13022  rotten       1\n",
      "13027   fresh       1\n",
      "13028   fresh       1\n",
      "13030   fresh       1\n",
      "13032   fresh       1\n",
      "13033   fresh       1\n",
      "13036   fresh       1\n",
      "13026   fresh       1\n",
      "27616   fresh       1\n",
      "\n",
      "[15561 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.02623694  0.00069849  0.05188384  0.00594742  0.04387859]\n",
      " [ 0.04345385  0.          0.          0.00661193  0.          0.        ]\n",
      " [ 0.02135266  0.00848788  0.00071898  0.01402524  0.01098371  0.00673189]\n",
      " ..., \n",
      " [ 0.03002721  0.02894077  0.00070541  0.01539482  0.02686553  0.00451604]\n",
      " [ 0.03844525  0.          0.00227036  0.02848203  0.02667402  0.0003269 ]\n",
      " [ 0.03560771  0.04045128  0.          0.02295952  0.          0.00537636]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model=NMF(n_components=6)\n",
    "model.fit(csr_mat)\n",
    "nmf_features=model.transform(csr_mat)\n",
    "print(nmf_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(nmf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026237</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.051884</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.043879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021353</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.010984</td>\n",
       "      <td>0.006732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.031075</td>\n",
       "      <td>0.026028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.019514</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.028253</td>\n",
       "      <td>0.057298</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.040371</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.019751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.025739</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.035553</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.034414</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.025647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048061</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.042627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029573</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.025028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>0.000664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.023759</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.032513</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.042477</td>\n",
       "      <td>0.028669</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.013069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.031335</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.000830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015497</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>0.004278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.020026</td>\n",
       "      <td>0.022784</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.020105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.032722</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.013878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.058188</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039090</td>\n",
       "      <td>0.018466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.022568</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.016391</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.002369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.020269</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.021062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045460</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.043993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>0.029334</td>\n",
       "      <td>0.023422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15532</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.066367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.037327</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15533</th>\n",
       "      <td>0.014817</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.046597</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.021895</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>0.019229</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>0.040711</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.032380</td>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>0.041316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>0.027536</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037516</td>\n",
       "      <td>0.019151</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15542</th>\n",
       "      <td>0.005350</td>\n",
       "      <td>0.018083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.039311</td>\n",
       "      <td>0.096260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>0.018999</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028380</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.129732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15545</th>\n",
       "      <td>0.037215</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15546</th>\n",
       "      <td>0.024492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.009691</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.002928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.045359</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>0.034652</td>\n",
       "      <td>0.041919</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.035636</td>\n",
       "      <td>0.014296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049443</td>\n",
       "      <td>0.032576</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031809</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554</th>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>0.038982</td>\n",
       "      <td>0.016568</td>\n",
       "      <td>0.030489</td>\n",
       "      <td>0.002993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15555</th>\n",
       "      <td>0.029916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.027246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.002097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>0.030027</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.004516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>0.035608</td>\n",
       "      <td>0.040451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15561 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "0      0.000000  0.026237  0.000698  0.051884  0.005947  0.043879\n",
       "1      0.043454  0.000000  0.000000  0.006612  0.000000  0.000000\n",
       "2      0.021353  0.008488  0.000719  0.014025  0.010984  0.006732\n",
       "3      0.040490  0.000000  0.000012  0.031075  0.026028  0.000000\n",
       "4      0.002255  0.000115  0.001012  0.019514  0.015501  0.001179\n",
       "5      0.020158  0.026664  0.028253  0.057298  0.007154  0.000218\n",
       "6      0.040371  0.006927  0.000885  0.019751  0.000000  0.010485\n",
       "7      0.022033  0.009763  0.000000  0.028107  0.000000  0.011873\n",
       "8      0.025739  0.000246  0.002478  0.035553  0.046781  0.000000\n",
       "9      0.034414  0.001882  0.000000  0.028246  0.005309  0.002402\n",
       "10     0.025647  0.000000  0.000000  0.000000  0.048061  0.000000\n",
       "11     0.042627  0.000000  0.000454  0.021417  0.020596  0.000000\n",
       "12     0.005591  0.000000  0.002326  0.000000  0.029573  0.000000\n",
       "13     0.025028  0.000000  0.000000  0.048822  0.013402  0.000664\n",
       "14     0.023759  0.001032  0.001293  0.000703  0.006617  0.000000\n",
       "15     0.000000  0.000000  0.003480  0.003318  0.030616  0.000000\n",
       "16     0.018145  0.032513  0.000210  0.020580  0.007704  0.001254\n",
       "17     0.042477  0.028669  0.001638  0.008928  0.007792  0.013069\n",
       "18     0.031335  0.010906  0.000000  0.013229  0.002631  0.000830\n",
       "19     0.000000  0.000000  0.000000  0.040534  0.000000  0.000000\n",
       "20     0.000000  0.015497  0.000733  0.015271  0.004590  0.000000\n",
       "21     0.009552  0.000000  0.000000  0.030603  0.019061  0.004278\n",
       "22     0.000000  0.000000  0.000000  0.001514  0.000373  0.000000\n",
       "23     0.020026  0.022784  0.000365  0.020105  0.000000  0.066209\n",
       "24     0.032722  0.067203  0.000000  0.000000  0.005321  0.013878\n",
       "25     0.058188  0.008860  0.000000  0.039090  0.018466  0.000000\n",
       "26     0.000000  0.000000  0.000313  0.000000  0.000000  0.118875\n",
       "27     0.022568  0.005637  0.001435  0.016391  0.002905  0.002369\n",
       "28     0.020269  0.042253  0.000947  0.003450  0.017112  0.021062\n",
       "29     0.003107  0.020053  0.000000  0.045460  0.002109  0.043993\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "15531  0.029334  0.023422  0.000000  0.013624  0.045668  0.000000\n",
       "15532  0.018696  0.066367  0.000000  0.021400  0.037327  0.000978\n",
       "15533  0.014817  0.022038  0.000128  0.046597  0.004804  0.000000\n",
       "15534  0.004774  0.021895  0.033435  0.015125  0.001272  0.001220\n",
       "15535  0.019229  0.011071  0.000000  0.025818  0.004320  0.000000\n",
       "15536  0.040711  0.010252  0.000474  0.032380  0.018289  0.000847\n",
       "15537  0.041316  0.000000  0.001229  0.021800  0.000000  0.000978\n",
       "15538  0.027536  0.005400  0.000000  0.046024  0.000000  0.000000\n",
       "15539  0.000000  0.000000  0.000077  0.048286  0.000000  0.000000\n",
       "15540  0.001835  0.007283  0.000000  0.037516  0.019151  0.001490\n",
       "15541  0.047199  0.000000  0.000174  0.001145  0.000000  0.000000\n",
       "15542  0.005350  0.018083  0.000000  0.010742  0.006901  0.011036\n",
       "15543  0.000114  0.039311  0.096260  0.000000  0.000000  0.006117\n",
       "15544  0.018999  0.002649  0.000000  0.028380  0.005031  0.129732\n",
       "15545  0.037215  0.006924  0.000000  0.001631  0.000000  0.000078\n",
       "15546  0.024492  0.000000  0.000276  0.025274  0.009691  0.000000\n",
       "15547  0.000611  0.003863  0.000000  0.011112  0.004279  0.002928\n",
       "15548  0.001051  0.001200  0.000570  0.000000  0.000609  0.000000\n",
       "15549  0.024380  0.016363  0.000219  0.045359  0.008019  0.000000\n",
       "15550  0.034652  0.041919  0.000667  0.035636  0.014296  0.000000\n",
       "15551  0.000000  0.037458  0.005827  0.005974  0.022397  0.000000\n",
       "15552  0.000000  0.000000  0.000000  0.049443  0.032576  0.000000\n",
       "15553  0.022082  0.000000  0.000374  0.000000  0.031809  0.000000\n",
       "15554  0.021634  0.014436  0.038982  0.016568  0.030489  0.002993\n",
       "15555  0.029916  0.000000  0.000876  0.027246  0.000000  0.004072\n",
       "15556  0.029868  0.007031  0.001011  0.009871  0.002720  0.002097\n",
       "15557  0.039856  0.000480  0.000000  0.009022  0.008292  0.000000\n",
       "15558  0.030027  0.028941  0.000705  0.015395  0.026866  0.004516\n",
       "15559  0.038445  0.000000  0.002270  0.028482  0.026674  0.000327\n",
       "15560  0.035608  0.040451  0.000000  0.022960  0.000000  0.005376\n",
       "\n",
       "[15561 rows x 6 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components_df=pd.DataFrame(model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 22417)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891      2.216783\n",
      "22091    0.831220\n",
      "10566    0.553755\n",
      "863      0.509981\n",
      "9365     0.327228\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(components_df.iloc[3].nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.984958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102567</td>\n",
       "      <td>0.112555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092953</td>\n",
       "      <td>0.880810</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.061723</td>\n",
       "      <td>0.460117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938685</td>\n",
       "      <td>0.344777</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.996790</td>\n",
       "      <td>0.062738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.507318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859614</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916930</td>\n",
       "      <td>0.397324</td>\n",
       "      <td>0.036676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.039935</td>\n",
       "      <td>0.114932</td>\n",
       "      <td>0.981916</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.040790</td>\n",
       "      <td>0.121167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.032244</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.994840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.076522</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.009355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874296</td>\n",
       "      <td>0.443705</td>\n",
       "      <td>0.172027</td>\n",
       "      <td>0.095601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.128051</td>\n",
       "      <td>0.924869</td>\n",
       "      <td>0.098370</td>\n",
       "      <td>0.143444</td>\n",
       "      <td>0.312759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685295</td>\n",
       "      <td>0.644257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339559</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.088852</td>\n",
       "      <td>0.262578</td>\n",
       "      <td>0.735907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617737</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.967961</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.135577</td>\n",
       "      <td>0.191516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987988</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.147962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.139965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662462</td>\n",
       "      <td>0.061203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985538</td>\n",
       "      <td>0.169456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.759170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.984901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165713</td>\n",
       "      <td>0.041767</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>0.018411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.016240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998287</td>\n",
       "      <td>0.046319</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.025105</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.099281</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.986746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.103634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.819375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.944280</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>0.289539</td>\n",
       "      <td>0.153766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15532</th>\n",
       "      <td>0.927819</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.095605</td>\n",
       "      <td>0.054903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15533</th>\n",
       "      <td>0.020225</td>\n",
       "      <td>0.990628</td>\n",
       "      <td>0.123951</td>\n",
       "      <td>0.048291</td>\n",
       "      <td>0.023491</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>0.128361</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.989607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.101096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>0.038295</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.024637</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.143753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.721532</td>\n",
       "      <td>0.519574</td>\n",
       "      <td>0.258803</td>\n",
       "      <td>0.233335</td>\n",
       "      <td>0.255444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.952034</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.291133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.214959</td>\n",
       "      <td>0.918542</td>\n",
       "      <td>0.120081</td>\n",
       "      <td>0.307028</td>\n",
       "      <td>0.032468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15542</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.035474</td>\n",
       "      <td>0.997719</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>0.906164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422675</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15545</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214255</td>\n",
       "      <td>0.824097</td>\n",
       "      <td>0.101856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15546</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.932282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>0.042501</td>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.216476</td>\n",
       "      <td>0.959962</td>\n",
       "      <td>0.059626</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>0.164441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.082563</td>\n",
       "      <td>0.989767</td>\n",
       "      <td>0.030782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>0.183898</td>\n",
       "      <td>0.279381</td>\n",
       "      <td>0.850008</td>\n",
       "      <td>0.099614</td>\n",
       "      <td>0.240902</td>\n",
       "      <td>0.312501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>0.999058</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554</th>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15555</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998672</td>\n",
       "      <td>0.051516</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>0.022905</td>\n",
       "      <td>0.121920</td>\n",
       "      <td>0.667232</td>\n",
       "      <td>0.083171</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>0.728743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545334</td>\n",
       "      <td>0.838219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>0.015703</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.003803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986546</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.162855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15561 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "0      0.073215  0.036170  0.984958  0.000000  0.102567  0.112555\n",
       "1      0.005654  0.000000  0.021850  0.014467  0.000000  0.999641\n",
       "2      0.000000  0.092953  0.880810  0.003863  0.061723  0.460117\n",
       "3      0.000000  0.938685  0.344777  0.000651  0.000000  0.000000\n",
       "4      0.013830  0.017659  0.996790  0.062738  0.000000  0.044396\n",
       "5      0.507318  0.000000  0.859614  0.060765  0.000000  0.000000\n",
       "6      0.000000  0.916930  0.397324  0.036676  0.000000  0.005326\n",
       "7      0.039935  0.114932  0.981916  0.068491  0.040790  0.121167\n",
       "8      0.032244  0.037400  0.994840  0.000000  0.000000  0.088631\n",
       "9      0.005677  0.004269  0.076522  0.996680  0.025193  0.009355\n",
       "10     0.999995  0.000000  0.000000  0.003193  0.000000  0.000000\n",
       "11     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "12     0.000000  0.047599  0.998320  0.000000  0.000000  0.033045\n",
       "13     0.000000  0.000000  0.874296  0.443705  0.172027  0.095601\n",
       "14     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "15     0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "16     0.012198  0.128051  0.924869  0.098370  0.143444  0.312759\n",
       "17     0.000000  0.685295  0.644257  0.000000  0.339559  0.001765\n",
       "18     0.088852  0.262578  0.735907  0.000000  0.617737  0.000000\n",
       "19     0.000000  0.089230  0.967961  0.005451  0.135577  0.191516\n",
       "20     0.000000  0.000000  0.987988  0.041833  0.015418  0.147962\n",
       "21     0.008996  0.000000  0.023111  0.000000  0.999692  0.000000\n",
       "22     0.139965  0.000000  0.733354  0.000000  0.662462  0.061203\n",
       "23     0.000000  0.000000  0.985538  0.169456  0.000000  0.000000\n",
       "24     0.006446  0.010077  0.759170  0.000000  0.650783  0.000000\n",
       "25     0.984901  0.000000  0.165713  0.041767  0.020647  0.018411\n",
       "26     0.016240  0.000000  0.998287  0.046319  0.031837  0.000000\n",
       "27     0.001507  0.025105  0.998821  0.041517  0.000000  0.000000\n",
       "28     0.099281  0.075363  0.986746  0.000000  0.007480  0.103634\n",
       "29     0.819375  0.000000  0.573258  0.000000  0.000000  0.000000\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "15531  0.000000  0.027956  0.944280  0.008774  0.289539  0.153766\n",
       "15532  0.927819  0.014126  0.324431  0.146774  0.095605  0.054903\n",
       "15533  0.020225  0.990628  0.123951  0.048291  0.023491  0.000000\n",
       "15534  0.000000  0.000000  0.990991  0.038213  0.128361  0.000000\n",
       "15535  0.000000  0.000000  0.998164  0.000000  0.000000  0.060572\n",
       "15536  0.034250  0.096213  0.989607  0.000000  0.005285  0.101096\n",
       "15537  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "15538  0.038295  0.005587  0.988549  0.024637  0.001131  0.143753\n",
       "15539  0.150859  0.721532  0.519574  0.258803  0.233335  0.255444\n",
       "15540  0.014776  0.092876  0.952034  0.005357  0.291133  0.000000\n",
       "15541  0.018239  0.214959  0.918542  0.120081  0.307028  0.032468\n",
       "15542  0.000000  0.000000  0.999892  0.000000  0.000000  0.014674\n",
       "15543  0.018166  0.035474  0.997719  0.011314  0.053299  0.000000\n",
       "15544  0.906164  0.000000  0.422675  0.014609  0.000000  0.000000\n",
       "15545  0.000000  0.214255  0.824097  0.101856  0.000000  0.514378\n",
       "15546  0.000000  0.900747  0.434343  0.000000  0.000000  0.000000\n",
       "15547  0.107171  0.160349  0.932282  0.000000  0.000000  0.306026\n",
       "15548  0.042501  0.252801  0.959838  0.000000  0.000000  0.114000\n",
       "15549  0.018423  0.216476  0.959962  0.059626  0.026007  0.164441\n",
       "15550  0.001223  0.082563  0.989767  0.030782  0.000000  0.112229\n",
       "15551  0.183898  0.279381  0.850008  0.099614  0.240902  0.312501\n",
       "15552  0.999058  0.040837  0.000000  0.014705  0.000000  0.000000\n",
       "15553  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "15554  0.005346  0.000000  0.999106  0.000000  0.000000  0.041933\n",
       "15555  0.000000  0.998672  0.051516  0.000350  0.000000  0.000000\n",
       "15556  0.022905  0.121920  0.667232  0.083171  0.037789  0.728743\n",
       "15557  0.000000  0.999976  0.000000  0.000000  0.006975  0.000000\n",
       "15558  0.000000  0.545334  0.838219  0.000000  0.000000  0.000000\n",
       "15559  0.006844  0.950462  0.308880  0.015703  0.030127  0.003803\n",
       "15560  0.000000  0.000000  0.986546  0.014358  0.162855  0.000000\n",
       "\n",
       "[15561 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "norm_features=normalize(nfm_features)\n",
    "df=pd.DataFrame(norm_features)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarities=df.dot(df.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3       1.000000\n",
       "8164    0.999999\n",
       "2855    0.999998\n",
       "749     0.999994\n",
       "8824    0.999987\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, y = make_xy(critics, n_vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "#your turn. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model=KMeans(n_clusters=2)\n",
    "model.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct=pd.crosstab(labels,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1403</td>\n",
       "      <td>2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2909</td>\n",
       "      <td>4335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0     1\n",
       "row_0            \n",
       "0      1403  2245\n",
       "1      2909  4335"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
